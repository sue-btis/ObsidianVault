#Machine_Learning #Python #Calculus

[[2 Linear Algebra]]
[[3 Multivariable Calculus]]
# Linear Regression: Ordinary Least Squares vs Gradient Descent

Linear regression models the relationship between one or more predictor variables and a target variable. After learning **simple** and **multiple** linear regression, it‚Äôs important to understand how we solve them: analytically using **Ordinary Least Squares (OLS)** or algorithmically using **Gradient Descent (GD)**.

## ü§î OLS vs Gradient Descent

| Feature              | OLS                          | Gradient Descent (GD)         |
|----------------------|-------------------------------|-------------------------------|
| Type                 | Analytical                    | Iterative                     |
| Exact solution       | Yes                           | No (approximation)            |
| Speed on small data  | Fast                          | Slower                        |
| Speed on big data    | Slower (matrix ops expensive) | Faster                        |
| Requires tuning      | No                            | Yes (learning rate, epochs)   |
| Susceptible to       | Multicollinearity             | Learning rate issues          |

---

## üìå Objective

Minimize the **loss** (usually the **sum of squared errors**) between the predicted and actual values:

$$
	ext{Loss} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

---

## üßÆ Ordinary Least Squares (OLS)

OLS solves linear regression analytically using **matrix algebra**. The goal is to find:

$$
\beta = (X^T X)^{-1} X^T y
$$

Where:
- $X$ is the matrix of features (including a column of 1s for the intercept)
- $y$ is the vector of outputs
- $\beta$ is the vector of parameters: $[b, m_1, m_2, ..., m_n]$
- $X^T$ is the transpose of $X$

> üß† Note: If $X^T X$ is not invertible (due to multicollinearity), OLS fails.

## üîÅ Gradient Descent

**Gradient Descent** finds the parameters **iteratively**, minimizing the same loss function:

$$
\text{Loss}(m, b) = \sum_{i=1}^{N} (y_i - (mx_i + b))^2
$$

It updates parameters using the gradient:

$$
b \leftarrow b - \eta \cdot \frac{\partial \text{Loss}}{\partial b}
$$

$$
m \leftarrow m - \eta \cdot \frac{\partial \text{Loss}}{\partial m}
$$

Where $\eta$ is the **learning rate**.

### Gradients:

$$
\frac{\partial \text{Loss}}{\partial b} = -\frac{2}{N} \sum (y_i - (mx_i + b))
$$

$$
\frac{\partial \text{Loss}}{\partial m} = -\frac{2}{N} \sum x_i (y_i - (mx_i + b))
$$

### üîÅ Steps

1. Initialize $m$ and $b$ randomly
2. Update $m$ and $b$ using the gradients
3. Repeat until convergence (i.e., loss stops decreasing)

---

## üö¶ Learning Rate & Convergence

- If $\eta$ is **too small** ‚Üí learning is slow
- If $\eta$ is **too large** ‚Üí may overshoot the minimum

Convergence happens when the loss stops decreasing significantly.

---
## üõ† Example: OLS vs Gradient Descent en Scikit-Learn


### üìä Dataset

- **x**: Studied hours
- **y**: Clasification

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Datos simples
x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([1.5, 2.0, 3.5, 3.7, 4.5])

# Separar en conjunto de entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=0)
```

---

### üßÆ Ordinary Least Squares (OLS)

```python
from sklearn.linear_model import LinearRegression

# Crear y entrenar el modelo
ols_model = LinearRegression()
ols_model.fit(x_train, y_train)

# Par√°metros
m_ols = ols_model.coef_[0]
b_ols = ols_model.intercept_
print(f"Pendiente OLS: {m_ols}")
print(f"Intersecci√≥n OLS: {b_ols}")

# Predicciones
y_pred_ols = ols_model.predict(x)

# Gr√°fico
plt.scatter(x, y, label="Datos reales")
plt.plot(x, y_pred_ols, color="red", label="OLS")
plt.legend()
plt.xlabel("Horas de estudio")
plt.ylabel("Calificaci√≥n")
plt.title("Regresi√≥n Lineal (OLS)")
plt.show()
```

---

### üîÅ Gradient Descent (SGDRegressor)

```python
from sklearn.linear_model import SGDRegressor

# Crear y entrenar el modelo con regresi√≥n por descenso de gradiente
sgd_model = SGDRegressor(max_iter=1000, learning_rate="invscaling", eta0=0.01)
sgd_model.fit(x_train, y_train)

# Par√°metros
m_sgd = sgd_model.coef_[0]
b_sgd = sgd_model.intercept_[0]
print(f"Pendiente SGD: {m_sgd}")
print(f"Intersecci√≥n SGD: {b_sgd}")

# Predicciones
y_pred_sgd = sgd_model.predict(x)

# Gr√°fico
plt.scatter(x, y, label="Datos reales")
plt.plot(x, y_pred_sgd, color="green", label="SGD")
plt.legend()
plt.xlabel("Horas de estudio")
plt.ylabel("Calificaci√≥n")
plt.title("Regresi√≥n Lineal (Gradient Descent)")
plt.show()
```


---
### üìè Evaluaci√≥n del modelo

Para saber qu√© tan bien predice el modelo, usamos m√©tricas de error:

- $R^2$: Proporci√≥n de la varianza explicada por el modelo (m√°ximo: 1.0).
- MAE: Error absoluto promedio.
- MSE: Error cuadr√°tico medio.
- RMSE: Ra√≠z cuadrada del MSE (mismo orden que la variable objetivo).

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Evaluar el modelo OLS en el conjunto de prueba
y_test_pred_ols = ols_model.predict(x_test)

r2 = r2_score(y_test, y_test_pred_ols)
mae = mean_absolute_error(y_test, y_test_pred_ols)
mse = mean_squared_error(y_test, y_test_pred_ols)
rmse = np.sqrt(mse)

print("üîç Evaluaci√≥n OLS:")
print(f"R2 Score: {r2:.4f}")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.2f}")
```

---

> Una **R¬≤ alta** (cercana a 1) indica buen desempe√±o.  
> **MAE y RMSE bajos** significan menor error de predicci√≥n.
