#Machine_Learning 
#Supervised 
#Ensemble_model

# ğŸŒ² Random Forest: Classifier & Regressor

## ğŸ“Œ What is a Random Forest?

**Random Forest** is an **ensemble learning** method that builds **multiple decision trees** and combines their outputs to improve prediction performance.

> ğŸ¯ Itâ€™s like asking a crowd of decision trees and letting them vote.

- **Classifier**: predicts categories (e.g., spam or not spam)
- **Regressor**: predicts continuous values (e.g., house price)

---

## ğŸ” Why Use Random Forest?

- âœ… Handles both classification and regression
- âœ… Reduces overfitting (compared to a single decision tree)
- âœ… Works well with high-dimensional and noisy data
- âœ… Measures feature importance
- âœ… Supports missing values (with adjustments)

---

## ğŸ§  How Does It Work?

### 1. **Bootstrapping (Bagging)**

Each tree is trained on a **random sample with replacement** of the training data.

```text
Dataset â†’ [Tree 1] + [Tree 2] + ... + [Tree N]
       â†˜ each tree sees a different sample
```

### 2. **Random Feature Selection**

At each split in a tree, only a **random subset of features** is considered (not all of them). This increases diversity between trees.

### 3. **Aggregation**

- **Classification** â†’ majority vote across trees
- **Regression** â†’ average of predictions

---

## ğŸ“ˆ Random Forest in Python (Classifier)

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## ğŸ”¢ Random Forest Regressor

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("MSE:", mse)
```

---

## ğŸŒ¿ Hyperparameters to Tune

| Parameter         | Description                              |
|------------------|------------------------------------------|
| `n_estimators`    | Number of trees in the forest             |
| `max_depth`       | Maximum depth of each tree                |
| `max_features`    | Max number of features per split          |
| `min_samples_split` | Min samples required to split a node  |
| `bootstrap`       | Whether to use bootstrapped samples       |

---

## ğŸ“Š Feature Importance

Random Forest provides feature importance out-of-the-box:

```python
import pandas as pd
import matplotlib.pyplot as plt

importances = model.feature_importances_
features = X.columns

pd.Series(importances, index=features).sort_values().plot(kind='barh')
plt.title("Feature Importance")
plt.show()
```

---

## âš–ï¸ Pros vs Cons

| âœ… Pros                                | âš ï¸ Cons                          |
|----------------------------------------|----------------------------------|
| Works with mixed data types            | Less interpretable than a single tree |
| Handles missing values & outliers      | Slower training for large datasets |
| Reduces overfitting (via bagging)      | Large model size                 |
