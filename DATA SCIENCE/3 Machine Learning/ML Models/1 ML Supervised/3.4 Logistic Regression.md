#Machine_Learning 
#Supervised_Learning
# ğŸ” Logistic Regression 

## ğŸ§  Assumptions of Logistic Regression (What the model expects)

Think of logistic regression like baking a cake: it works best with the right ingredients.

1. **Binary outcome**: Your target should be 0 or 1.
2. **Independent observations**: Each row of data should be separate.
3. **Linear log-odds**: The relationship between inputs and the *log of the odds* of the outcome should be roughly linear.
4. **No strongly correlated inputs**: Donâ€™t feed the model repeated or very similar columns.
5. **Enough data**: More data = more reliable predictions.

---


## ğŸ¯ What is Logistic Regression?

Despite its name, **logistic regression** is used for **binary classification**. It predicts the probability that an instance belongs to a class (e.g., 0 or 1).

### ğŸ§  Formula:

$$
P(y = 1 | x) = \frac{1}{1 + e^{-(b + m_1 x_1 + m_2 x_2 + ... + m_n x_n)}}
$$

Where:

- $P$ is the predicted probability.
- $x_i$ are the input features.
- $b$ is the intercept.
- $m_i$ are the coefficients.
- The function $\frac{1}{1 + e^{-z}}$ is called the **sigmoid** or **logistic function**.

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-27.png" alt="Mi Imagen" width="350">
</div>

---

## ğŸ§ª Example using Scikit-Learn

```python
import seaborn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import codecademylib3

# Cargar el dataset
transactions = pd.read_csv('transactions_modified.csv')

# Â¿CuÃ¡ntas transacciones fraudulentas hay?
print("Fraudes:", len(transactions[transactions["isFraud"] == 1]))

# EstadÃ­sticas descriptivas de la columna 'amount'
print(transactions["amount"].describe())
plt.hist(transactions['amount'], bins=100)
plt.show()

# Crear columna isPayment
transactions["isPayment"] = transactions["type"].apply(lambda x: 1 if x in ["PAYMENT", "DEBIT"] else 0)

# Crear columna isMovement
transactions["isMovement"] = transactions["type"].apply(lambda x: 1 if x in ["CASH_OUT", "TRANSFER"] else 0)

# Crear columna accountDiff (diferencia entre origen y destino)
transactions["accountDiff"] = abs(transactions["oldbalanceOrg"] - transactions["newbalanceOrig"])

# Definir variables predictoras y etiqueta
X = transactions[["amount", "isPayment", "isMovement", "accountDiff"]]
y = transactions["isFraud"]

# Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Normalizar los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Entrenar el modelo
LRM = LogisticRegression()
LRM.fit(X_train, y_train)

# Evaluar el modelo
y_predict = LRM.predict(X_test)
print("Accuracy en test:", LRM.score(X_test, y_test))
print("Coeficientes del modelo:", LRM.coef_)
```

---

## ğŸ§¾ Confusion Matrix

The confusion matrix shows how well the model classifies positive and negative instances.

|                | Predicted: 0 | Predicted: 1 |
|----------------|---------------|---------------|
| **Actual: 0**  | True Negative | False Positive |
| **Actual: 1**  | False Negative | True Positive |

```python
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
```

---

## ğŸ“Š Evaluation Metrics

Derived from the confusion matrix:

- **Accuracy**: Proportion of total predictions that were correct.

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

- **Precision**: Of the predicted positives, how many were actually positive?

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

- **Recall (Sensitivity)**: Of the actual positives, how many were identified?

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

- **F1 Score**: Harmonic mean of precision and recall.

$$
F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$

```python
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
```

---

## âœ… When to Use Logistic Regression?

- When the target is binary (0 or 1).
- It's fast, interpretable, and works well with linearly separable data.
- A strong baseline for more complex classification models.

---

## âš™ï¸ Hyperparameters (Settings before training)

These are like the knobs you can adjust **before training** to help your model perform better.

| Hyperparameter | What it controls |
|----------------|------------------|
| `C`            | Regularization strength (smaller = simpler model) |
| `penalty`      | Type of regularization (`l1`, `l2`) |
| `solver`       | Algorithm used to find the best model |
| `class_weight` | How much weight to give each class (especially useful if data is unbalanced) |

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    C=0.1,
    penalty='l2',
    solver='liblinear',
    class_weight='balanced'  # helps with imbalance
)
```

ğŸ“ *scikit-learn calls these â€œparametersâ€ in the function, but theyâ€™re really hyperparameters.*

---

## âš–ï¸ Adjusting the Prediction Threshold

By default, if the model gives a probability â‰¥ 0.5, it predicts class 1. But you can **lower or raise this threshold** to change how strict the model is.

```python
y_proba = model.predict_proba(X_test)[:, 1]
y_pred_custom = (y_proba > 0.3).astype(int)  # More sensitive to catching positives
```

ğŸ¯ Lower threshold = more positives found (but more false alarms too).

---

## ğŸ“ˆ ROC Curve and AUC â€“ Visual Scorecard

### ğŸ” ROC Curve
- Shows how well your model separates the two classes.
- Plots True Positive Rate vs False Positive Rate.
- More curve = better model.

### ğŸ“ AUC (Area Under the Curve)
- A score from 0.5 to 1.0
  - 0.5 = guessing
  - 1.0 = perfect
- Higher is better

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], '--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()
```

---

## âš ï¸ Class Imbalance â€“ When You Have Way More of One Class

Imagine youâ€™re predicting disease, and only 1 out of 100 people has it. A model could say "nobody has it" and still be 99% accurate â€” but useless!

### âœ… Positivity Rate

$$
\text{Positivity Rate} = \frac{\text{Positive Cases}}{\text{Total Cases}}
$$

If this number is far from 0.5, you might have imbalance problems.

---

## ğŸ§ª How to Fix Imbalance

### 1. ğŸŸ¨ Use Stratified Sampling

Make sure train/test sets keep the same class ratio.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
```

---

### 2. ğŸŸ¦ Balance Class Weights

Give more "importance" to the smaller class so the model pays attention to it.

```python
model = LogisticRegression(class_weight='balanced')
```

---

### 3. ğŸŸ¥ Undersampling / Oversampling

- **Oversampling** = Duplicate samples of the small class
- **Undersampling** = Remove some of the big class

These are advanced techniques and not always necessary, but they help.

---

## âœ… Summary Table

| Topic            | Simple Explanation |
|------------------|--------------------|
| Assumptions      | Data should be binary, independent, and balanced |
| Hyperparameters  | You control model flexibility and training options |
| Threshold        | Adjust how strict your predictions are |
| ROC/AUC          | Visual + numerical score of how well your model separates classes |
| Class imbalance  | Use stratification or weighting to help minority class |