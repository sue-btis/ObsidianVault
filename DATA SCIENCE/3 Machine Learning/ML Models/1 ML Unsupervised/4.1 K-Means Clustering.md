
#Machine_Learning
#Unsupervised_Learning
# 📊 K-Means Clustering

## 📌 What is Clustering?

**Clustering** is an **unsupervised learning** technique used to group data points so that:
- Samples in the **same group (cluster)** are **similar**
- Samples in **different groups** are **dissimilar**

### Key Questions
1. **How many clusters (`k`) should we choose?**
2. **How do we measure similarity between data points?**

---

## 🚀 What is K-Means?

**K-Means** is one of the most widely used clustering algorithms.

- **k**: Number of clusters
- **Means**: Refers to the **centroids**, or the average location of data points in each cluster

### 🧠 K-Means Algorithm (Iterative Process)

1. Randomly initialize **k centroids**
2. Assign each data point to its **nearest centroid**
3. Recalculate centroids as the **mean** of assigned points
4. Repeat steps 2–3 until **convergence**

> ✅ **Convergence** means: data points no longer switch clusters, and centroids stabilize.

---

## 🔄 Training vs Inference

- **Training**: The iterative process to find the best cluster centroids.
- **Inference**: After training, we can assign new data points to the closest cluster **without retraining**.

---

## 📉 Choosing the Right Number of Clusters

The performance of K-Means heavily depends on the value of **k**.

### ⚙️ Inertia

- **Inertia**: Sum of squared distances from each point to its assigned cluster’s centroid.
- Lower inertia means **tighter clusters**.

```python
print(model.inertia_)
```

### 📈 The Elbow Method

- Plot **inertia vs k**.
- Look for a point where the rate of decrease sharply slows down → the "**elbow**".
- This value of `k` is often a good trade-off between **model complexity** and **tight clustering**.

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertias = []
ks = range(1, 10)

for k in ks:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X)
    inertias.append(model.inertia_)

plt.plot(ks, inertias, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")
plt.title("Elbow Method for Optimal k")
plt.show()
```

> 📌 In the Iris dataset, the elbow usually appears at `k = 3`, which matches the number of Iris species.

---

## 📊 Visualizing K-Means

After fitting the model, we can visualize how the data was grouped:

```python
import seaborn as sns

model = KMeans(n_clusters=3, random_state=42)
model.fit(X)

labels = model.labels_

sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette="Set2")
```

> 🎯 Use only the first two features if your dataset has more than two dimensions, or apply dimensionality reduction (e.g., PCA).
