#Machine_Learning 
# üìê Feature Engineering: Regularization 

Regularization helps us build **better predictive models** by preventing **overfitting** and improving **generalization**.

---

## ü§î Why Regularize?

### Overfitting vs. Underfitting
- **Overfitting**: Model learns the training data *too well*, capturing noise. It performs poorly on new data.
- **Underfitting**: Model is too simple and doesn't capture the true pattern.
- Regularization **adds a penalty** to the model‚Äôs complexity to reduce overfitting.

---

## üìâ The Loss Function with Regularization

Regularization adds a **penalty** term to the loss function:

- **Without regularization**:
  $$
  Loss = Error(y_{true}, y_{pred})
  $$

- **With regularization**:
  $$
  Loss = Error(y_{true}, y_{pred}) + \lambda \times \text{RegularizationTerm}
  $$

Where:
- $\lambda$ (alpha in `sklearn`) controls the penalty strength.
- A higher $\lambda$ reduces model complexity more aggressively.

---

## üßÆ L1 Regularization (Lasso)

- Uses the **absolute value** of coefficients:
  $$
  L1 = \sum |w_i|
  $$
- Can shrink some coefficients **exactly to 0** ‚áí automatic feature selection.
- Works well with **sparse data**.

### In `scikit-learn`:

```python
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

---

## üßÆ L2 Regularization (Ridge)

- Uses the **squared value** of coefficients:
  $$
  L2 = \sum w_i^2
  $$
- Shrinks coefficients smoothly ‚áí keeps all features.
- More stable when features are correlated.

### In `scikit-learn`:

```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
```

---

## üîé Tuning Hyperparameters with GridSearchCV

## üîÅ Lasso Regularization


To choose the best $\alpha$ (regularization strength), we use **cross-validation**.

```python
from sklearn.model_selection import GridSearchCV
import numpy as np

alpha_range = np.logspace(-4, 2, 100)
param_grid = {'alpha': alpha_range}

model = GridSearchCV(Lasso(), param_grid, scoring='neg_mean_squared_error', cv=5)
model.fit(X_train, y_train)

print(model.best_params_)
```

---

## üîÅ Ridge Regularization

```python
from sklearn.linear_model import Ridge

alpha_values = np.logspace(-2, 4, 100)
params = {'alpha': alpha_values}

ridge_grid = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)
ridge_grid.fit(X_train, y_train)

print(ridge_grid.best_params_)
```

---

## üß† Bias-Variance Tradeoff

| **Alpha (Œª)** | **Model Complexity** | **Bias** | **Variance** | **Risk** |
|---------------|----------------------|----------|--------------|----------|
| Small (‚Üí0)    | High                 | Low      | High         | Overfit  |
| Large         | Low                  | High     | Low          | Underfit |

Goal: Find the ‚Äújust right‚Äù (Goldilocks) zone where test error is lowest.

---

## ‚úÖ Regularization in Logistic Regression

Use `LogisticRegression()` and choose:
- `penalty = 'l1'` for Lasso
- `penalty = 'l2'` for Ridge (default)
- `C = 1/alpha` ‚áí **higher C = less regularization**

```python
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(penalty='l1', solver='liblinear', C=0.5)
logreg.fit(X_train, y_train)
```

---

## üîç Logistic + GridSearchCV

```python
from sklearn.model_selection import GridSearchCV

C_range = np.logspace(-3, 2, 100)
param_grid = {'C': C_range}

log_grid = GridSearchCV(LogisticRegression(penalty='l1', solver='liblinear'),
                        param_grid, scoring='accuracy', cv=5)
log_grid.fit(X_train, y_train)

print(log_grid.best_params_)
```

---

## üöÄ LogisticRegressionCV (Shortcut)

```python
from sklearn.linear_model import LogisticRegressionCV

model = LogisticRegressionCV(Cs=np.logspace(-3, 2, 100),
                             penalty='l2',
                             cv=5,
                             scoring='accuracy',
                             max_iter=10000)
model.fit(X, y)
print(model.C_)
```

---

## üìå Summary Table

| Method      | Model Type     | Parameter | sklearn class           | Notes                       |
|-------------|----------------|-----------|--------------------------|-----------------------------|
| Lasso       | Linear         | alpha     | `Lasso()`                | Feature selection           |
| Ridge       | Linear         | alpha     | `Ridge()`                | Smooth shrinkage            |
| L1          | Logistic       | C         | `LogisticRegression()`   | Use solver='liblinear'      |
| L2          | Logistic       | C         | `LogisticRegression()`   | Default                     |
| GridSearchCV| Linear/Logistic| alpha/C   | `GridSearchCV()`         | Exhaustive tuning           |
| CV Shortcut | Logistic       | C         | `LogisticRegressionCV()` | Fast + efficient tuning     |


