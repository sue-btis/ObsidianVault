#Math #Linear_Algebra
[[2 Linear Algebra with Py]]

## Course: #Mathematics_for_Machine_Learning
## Platform: Coursera

# ğŸ“ Vectores 
----
## ğŸ“Š Representar Datos como Vectores

*  RepresentaciÃ³n como vector columna
* En matemÃ¡ticas y programaciÃ³n, se suele representar asÃ­:

$$
\mathbf{r} = \begin{bmatrix} a \\ b \end{bmatrix}
$$

* Esto hace mÃ¡s fÃ¡cil calcular y visualizar en Ã¡lgebra lineal y computadoras.

### Ejemplo: Alturas en una poblaciÃ³n

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-10.png" alt="Mi Imagen" width="500">
</div>

- Se agrupan por rangos de 2.5 cm para formar un histograma.
- Esto se transforma en un vector de frecuencias:

$$
\mathbf{f} = \begin{bmatrix}
f_{150.0,152.5} \\
f_{152.5,155.0} \\
f_{155.0,157.5} \\
f_{157.5,160.0} \\
f_{160.0,162.5} \\
\vdots
\end{bmatrix}
$$

- Cada componente indica cuÃ¡ntas personas hay en ese rango.

---

## ğŸ“ˆ Modelar Datos con DistribuciÃ³n Normal

### QuÃ© es una curva normal (o gaussiana)

- Una curva en forma de campana que describe cÃ³mo se distribuyen los datos.
- ML, se usa para modelar probabilidades o hacer supuestos sobre la forma de los datos.
- Tiene dos parÃ¡metros clave:
  - $\mu$: media (centro de la curva)
  - $\sigma$: desviaciÃ³n estÃ¡ndar (quÃ© tan ancha es la curva)

$$
g(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

### Vector de parÃ¡metros:

$$
\mathbf{p} = \begin{bmatrix} \mu \\ \sigma \end{bmatrix}
$$

### PredicciÃ³n de frecuencias:

* Usamos una **distribuciÃ³n normal** con parÃ¡metros $\mu$ y $\sigma$ para generar una predicciÃ³n de cÃ³mo se distribuirÃ­an los datos si siguieran esa curva.

$$
\mathbf{g}_\mathbf{p} = \begin{bmatrix}
g_{150.0,152.5} \\
g_{152.5,155.0} \\
\vdots
\end{bmatrix}
$$

---

## ğŸ“ Medida de Ajuste del Modelo

- Se compara lo que predice el modelo con los datos reales.
- **Residual** = diferencia entre datos reales y predicciÃ³n.
- Se calcula el **SSR** (suma de residuos al cuadrado):

$$
SSR(p) = \|\mathbf{f} - \mathbf{g}_\mathbf{p}\|^2
$$

ğŸ” Objetivo: ajustar $\mu$ y $\sigma$ para que SSR sea lo mÃ¡s pequeÃ±o posible.

---

## ğŸ§­ Mapas de Contorno para OptimizaciÃ³n

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-11.png" alt="Mi Imagen" width="500">
</div>

- Cada punto $\mathbf{p} = [\mu, \sigma]$ genera una curva diferente.
- Se crea una superficie de valores SSR.
- En un mapa de contorno, las lÃ­neas indican niveles de SSR.
- El mejor modelo estÃ¡ en el **mÃ­nimo global** de esa superficie.

$$
\Delta\mathbf{p} = \text{direcciÃ³n que mejora el ajuste del modelo}
$$

---
## ğŸ” Operaciones BÃ¡sicas con Vectores

### **Suma de vectores**

$$
\mathbf{r} = [3, 2], \quad \mathbf{s} = [1, 4]
$$
$$
\mathbf{r} + \mathbf{s} = [3 + 1, 2 + 4] = [4, 6]
$$

### **MultiplicaciÃ³n por un nÃºmero (escalar)**

$$
\mathbf{r} = [3, 2], \quad a = 2
$$
$$
a\mathbf{r} = 2 \cdot [3, 2] = [6, 4]
$$
### **Resta de vectores**
$$
\mathbf{r} = [3, 2], \quad \mathbf{s} = [1, 4]
$$
$$
\mathbf{r} - \mathbf{s} = [3 - 1, 2 - 4] = [2, -2]
$$
TambiÃ©n se puede expresar como:

$$
\mathbf{r} - \mathbf{s} = \mathbf{r} + (-1) \cdot \mathbf{s} = [3, 2] + [-1, -4] = [2, -2]
$$
---

## ğŸ“ Longitud (o Magnitud) de un Vector

Ejemplo fÃ­sico: la **velocidad de un coche** en lÃ­nea recta puede representarse con un vector. Su longitud es la rapidez total, sin importar la direcciÃ³n.

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-12.png" alt="Mi Imagen" width="250">
</div>
Si:
$$
\mathbf{r} = a\hat{i} + b\hat{j}
$$
Entonces:
$$
\|\mathbf{r}\| = \sqrt{a^2 + b^2}
$$

ğŸ“Œ Esto se deriva del teorema de PitÃ¡goras. Aplica incluso si las componentes tienen unidades distintas (ej. tiempo, dinero, distancia).

---
## âœ´ï¸ Producto Punto (Dot Product)

El **producto punto** es una forma de "multiplicar" vectores que **devuelve un nÃºmero escalar**.

ğŸ“Œ **Â¿QuÃ© mide?**  
Mide cuÃ¡nto **uno de los vectores contribuye en la direcciÃ³n del otro**. Si los vectores fueran fuerzas o velocidades, el producto punto te dice **cuÃ¡nta fuerza o movimiento va en la misma direcciÃ³n**.

### ğŸ§  Ejemplo intuitivo:
Imagina que caminas con viento:

- Si el viento va en la **misma direcciÃ³n** que tÃº â‡’ te empuja (dot positivo).
- Si el viento sopla **de lado** â‡’ no te ayuda ni estorba (dot = 0).
- Si el viento viene **de frente** â‡’ te frena (dot negativo).

### ğŸ§® FÃ³rmula:
En 2D:

$$
\mathbf{r} \cdot \mathbf{s} = r_1s_1 + r_2s_2
$$

Cuando haces el dot product de un vector consigo mismo:

$$
\mathbf{r} \cdot \mathbf{r} = \|\mathbf{r}\|^2
$$

ğŸ“Œ Ãštil para:
- Obtener magnitudes sin usar raÃ­z cuadrada hasta el final.
- Detectar vectores nulos (si dot = 0).
---
## ğŸ“Œ Propiedades del Producto Punto

1. **Conmutativo**:  
   $$ \mathbf{r} \cdot \mathbf{s} = \mathbf{s} \cdot \mathbf{r} $$
2. **Distributivo sobre suma**:  
   $$ \mathbf{r} \cdot (\mathbf{s} + \mathbf{t}) = \mathbf{r} \cdot \mathbf{s} + \mathbf{r} \cdot \mathbf{t} $$
3. **Asociativo con escalares**:  
   $$ \mathbf{r} \cdot (a\mathbf{s}) = a(\mathbf{r} \cdot \mathbf{s}) $$
âš™ï¸ Estas propiedades lo hacen **fÃ¡cil de usar en Ã¡lgebra lineal, programaciÃ³n y simulaciones fÃ­sicas**.

---
## ğŸ§  Ãngulo entre Vectores

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-14.png" alt="Mi Imagen" width="200">
</div>

La conexiÃ³n con el Ã¡ngulo entre vectores:
$$
\mathbf{r} \cdot \mathbf{s} = \|\mathbf{r}\| \cdot \|\mathbf{s}\| \cdot \cos(\theta)
$$
### ğŸ¯ Â¿QuÃ© nos dice?

- Si $( \theta = 0Â° )$: **idÃ©ntica direcciÃ³n** â†’ positivo
- Si $( \theta = 90Â° )$: **perpendiculares** â†’ dot = 0
- Si $( \theta = 180Â° )$: **direcciÃ³n opuesta** â†’  negativo

ğŸ“Œ Se usa para saber si dos movimientos, fuerzas o direcciones **se ayudan, se ignoran o se oponen**.

---
## ğŸ”¦ ProyecciÃ³n Escalar y Vectorial

- ğŸ”¢ ProyecciÃ³n escalar â†’ te dice â€œcuÃ¡nto de $\mathbf{r}$â€ hay en la direcciÃ³n de $\mathbf{b}_1$â€‹â€ â†’ Ãºtil para cambiar de base.
    
- â¡ï¸ ProyecciÃ³n vectorial â†’ te da directamente â€œla sombra de $\mathbf{r}$â€ sobre $\mathbf{b}_1$â€‹â€ â†’ Ãºtil si necesitas sumar los vectores proyectados.
<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-16.png" alt="Mi Imagen" width="300">
</div>
### ğŸ”¹ ProyecciÃ³n escalar
$$
\text{proj}_{\mathbf{r}}(\mathbf{s}) = \frac{\mathbf{r} \cdot \mathbf{s}}{\|\mathbf{r}\|}
$$

---
### ğŸ”¸ ProyecciÃ³n vectorial
$$
\text{Proj}_{\mathbf{r}}(\mathbf{s}) = \left( \frac{\mathbf{r} \cdot \mathbf{s}}{\mathbf{r} \cdot \mathbf{r}} \right) \mathbf{r}
$$
---
## ğŸ§© IntuiciÃ³n Final

ğŸ” El producto punto no es solo una fÃ³rmula:

- Es una herramienta para **medir alineaciÃ³n**.
- Sirve para saber **quÃ© tanto dos vectores trabajan juntos o se cancelan**.
- Es clave para **machine learning**, **fÃ­sica**, **3D graphics**, y mÃ¡s.

ğŸ’¡ **Cuando haces un dot product, estÃ¡s colapsando un vector sobre otro**. Te ayuda a **comparar direcciones, extraer componentes Ãºtiles** y entender cÃ³mo interactÃºan dos efectos.

---
# ğŸ§­ Cambios de Base, Proyecciones y Espacios Vectoriales

## ğŸ“Œ Â¿Por quÃ© importa la base?

Cuando usamos vectores, usualmente lo hacemos dentro de un **sistema de coordenadas**, definido por un conjunto de **vectores base**. Estos vectores base nos dicen cÃ³mo movernos en el espacio. Pero:

ğŸ§  **El vector existe independientemente de la base**. Solo cambia cÃ³mo lo describimos (sus "coordenadas").

---

## ğŸ” Cambiar de Base (de $\mathbf{e}$ a $\mathbf{b}$)

SupÃ³n que un vector $\mathbf{r} = [3, 4]$ estÃ¡ expresado con la base estÃ¡ndar $\{\hat{e}_1, \hat{e}_2\}$.

Ahora queremos reescribirlo usando otra base $\{\mathbf{b}_1, \mathbf{b}_2\}$, donde:

$\mathbf{b}_1 = [2, 1], \quad \mathbf{b}_2 = [-2, 4]$

âœ… Verificamos que $\mathbf{b}_1 \perp \mathbf{b}_2$:

$\mathbf{b}_1 \cdot \mathbf{b}_2 = 2(-2) + 1(4) = -4 + 4 = 0 \Rightarrow \text{Son ortogonales}$

Entonces podemos usar **proyecciones**:

$r_{b_1} = \frac{\mathbf{r} \cdot \mathbf{b}_1}{\|\mathbf{b}_1\|^2} = \frac{3(2) + 4(1)}{2^2 + 1^2} = \frac{10}{5} = 2$

$r_{b_2} = \frac{\mathbf{r} \cdot \mathbf{b}_2}{\|\mathbf{b}_2\|^2} = \frac{3(-2) + 4(4)}{(-2)^2 + 4^2} = \frac{10}{20} = 0.5$

ğŸ”„ Entonces:

$\mathbf{r} = 2 \cdot \mathbf{b}_1 + 0.5 \cdot \mathbf{b}_2$

ğŸ‘‰ Hemos convertido la representaciÃ³n de $\mathbf{r}$ desde la base $\mathbf{e}$ a la base $\mathbf{b}$.

---

## ğŸ“ Â¿QuÃ© es una Base?

Un **conjunto de vectores linealmente independientes** que:
- No pueden escribirse unos en funciÃ³n de otros.
- Juntos generan todo el espacio (span).

ğŸ”¢ El nÃºmero de vectores base = **dimensiÃ³n del espacio**.

- 1 vector independiente â†’ lÃ­nea (1D)
- 2 independientes â†’ plano (2D)
- 3 independientes â†’ espacio (3D)

ğŸ‘‰ Si aÃ±ades un vector y **no es combinaciÃ³n lineal** de los anteriores, creas una dimensiÃ³n nueva.

---

## ğŸ§  IntuiciÃ³n Aplicada a Datos

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-17.png" alt="Mi Imagen" width="300">
</div>
Imagina que tienes puntos de datos en 2D que **caen casi sobre una lÃ­nea**.

âœ… PodrÃ­as definir:
- Un eje nuevo "a lo largo de la lÃ­nea"
- Otro eje "perpendicular" (que mida la distancia desde la lÃ­nea)

ğŸ‘‰ Esto se parece mucho a **reducciÃ³n de dimensionalidad** (como en PCA):

- El eje de la lÃ­nea representa la **informaciÃ³n importante**
- El eje perpendicular mide el **ruido** o error

En redes neuronales:
- Las bases podrÃ­an representar "rasgos latentes" como **forma de nariz**, **tono de piel**, etc.
- El modelo aprende una nueva base Ãºtil para representar los datos.

---

## ğŸ”„ Base Natural vs. Base Aprendida

- Base natural: $[1, 0], [0, 1]$
- Base transformada: puede ser cualquier par de vectores independientes (aunque no ortogonales)

ğŸ‘‰ Cambiar la base puede:
- Alinear los datos para anÃ¡lisis mÃ¡s simple
- Reducir ruido
- Identificar patrones ocultos

---

# ğŸMatriz

QuÃ© tienen en comÃºn manzanas, plÃ¡tanos y matrices?

 Â¿cuÃ¡nto cuesta cada fruta si solo sabemos el total de la cuenta? Si compras 2 manzanas y 3 plÃ¡tanos por â‚¬8, y otro dÃ­a compras 10 manzanas y 1 plÃ¡tano por â‚¬13, puedes plantear el problema como un sistema de ecuaciones. Pero lo mÃ¡s poderoso es que **tambiÃ©n puedes resolverlo con matrices**.

---
## ğŸ§® Representando el problema como matriz

La forma matricial del sistema es:

$$
\begin{bmatrix} 2 & 3 \\ 10 & 1 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} 8 \\ 13 \end{bmatrix}
$$

Esta ecuaciÃ³n resume la idea de que una **matriz puede actuar sobre un vector** y devolver otro vector. AquÃ­, los precios $(a, b)$ de las frutas se transforman en los totales â‚¬8 y â‚¬13.

---

## ğŸ§­ Â¿QuÃ© hace realmente una matriz?

Visualmente, una matriz **transforma el espacio**: toma vectores base (como $\hat{e}_1 = [1, 0]$ y $\hat{e}_2 = [0, 1]$) y los lleva a nuevas posiciones. En este ejemplo:

<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-18.png" alt="Mi Imagen" width="300">
</div>

$$
\text{Matriz} = \begin{bmatrix} 2 & 3 \\ 10 & 1 \end{bmatrix} \Rightarrow \begin{cases} \hat{e}_1 \to [2, 10] \\ \hat{e}_2 \to [3, 1] \end{cases}
$$

Esto significa que el espacio se ha estirado, rotado o deformado, y cualquier vector serÃ¡ una combinaciÃ³n de esas nuevas direcciones.

---

## ğŸ¯ Propiedades clave de las matrices

Â¿Por quÃ© es Ãºtil esto? Porque estas operaciones **respetan la estructura lineal**:

- Escalado: $A(n \mathbf{r}) = n A(\mathbf{r})$
- Suma: $A(\mathbf{r} + \mathbf{s}) = A(\mathbf{r}) + A(\mathbf{s})$

Esto asegura que **la combinaciÃ³n lineal de vectores** se transforma en la misma combinaciÃ³n de sus transformados.

---

## ğŸ”„ Tipos de transformaciones

Las matrices permiten **describir visual y funcionalmente** los cambios espaciales. Cada tipo modifica el espacio de forma distinta:

| Tipo            | Matriz                                   | QuÃ© hace                                                            |
|-----------------|-------------------------------------------|---------------------------------------------------------------------|
| Identidad       | $\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$     | No cambia nada                                                      |
| Escalado        | $\begin{bmatrix}3 & 0 \\ 0 & 2\end{bmatrix}$     | Estira los ejes (x3 en x, x2 en y)                                 |
| Reflejo         | $\begin{bmatrix}-1 & 0 \\ 0 & 1\end{bmatrix}$    | Refleja sobre eje y                                                 |
| InversiÃ³n total | $\begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$   | Refleja ambos ejes (giro de 180Â°)                                   |
| Cizalla         | $\begin{bmatrix}1 & k \\ 0 & 1\end{bmatrix}$      | Desplaza filas paralelamente (paralelogramo)                        |
| RotaciÃ³n        | $\begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$ | Gira todo el espacio                                                 |

Cada transformaciÃ³n es una herramienta para modificar un objeto o conjunto de datos sin perder estructura.

---

## ğŸ§° ComposiciÃ³n de transformaciones

Al aplicar varias transformaciones, **el orden importa**. Si aplicas primero una rotaciÃ³n y luego un reflejo, obtendrÃ¡s un resultado diferente a hacerlo al revÃ©s.

Ejemplo:

- RotaciÃ³n $90^\circ$ CCW:
$$ A_1 = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} $$
- Reflejo vertical:
  $$ A_2 = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix} $$
- ComposiciÃ³n:
  $$ A_2 A_1 \neq A_1 A_2 $$

Esto nos lleva a un concepto clave: **la multiplicaciÃ³n de matrices no es conmutativa**.

---
## ğŸ§  Â¿Por quÃ© transformar vectores con matrices?

Aunque los **vectores representan datos**, las **matrices nos permiten cambiar la forma en que los observamos**.

Transformar datos no cambia su esencia, **cambia su perspectiva**. Al aplicar una matriz, podemos:

- ğŸ” **Descubrir patrones** ocultos al rotar o proyectar los datos.
    
- ğŸ“ **Reducir dimensiones**, conservando lo mÃ¡s importante (como en PCA).
    
- ğŸ§  **Prepararlos para modelos** que aprenden mejor en espacios especÃ­ficos.
    
- ğŸ“Š **Descorrelacionar variables** y facilitar el anÃ¡lisis.
    

> Es como ver una escultura desde otro Ã¡ngulo: **es la misma**, pero entiendes mejor su forma.

---


## ğŸ” IntroducciÃ³n a la matriz inversa

La **matriz inversa** $A^{-1}$ cumple:

$$
A^{-1} \cdot A = I
$$

Donde $I$ es la **matriz identidad**, es decir:

$$
I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
$$

Esta matriz deja todo igual al multiplicar: $I \cdot \vec{x} = \vec{x}$. Es el equivalente al nÃºmero 1 en la multiplicaciÃ³n escalar.

Si logramos encontrar $A^{-1}$, podemos resolver:

$$
\vec{r} = A^{-1} \cdot \vec{s}
$$

Esto permite encontrar $\vec{r}$ para **cualquier** vector de salida $\vec{s}$.

## âœ³ï¸ EliminaciÃ³n y sustituciÃ³n hacia atrÃ¡s

En lugar de calcular directamente la inversa, podemos resolver mediante:

1. **EliminaciÃ³n de filas (reducciÃ³n a forma escalonada o Echelon)**
2. **SustituciÃ³n hacia atrÃ¡s (Back-substitution)**

Ejemplo con sistema ampliado:

$$
\begin{bmatrix} 1 & 1 & 3 \\ 1 & 2 & 4 \\ 1 & 1 & 2 \end{bmatrix} \cdot \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} 15 \\ 21 \\ 13 \end{bmatrix}
$$

Mediante operaciones entre filas se reduce a:

$$
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} 5 \\ 4 \\ 2 \end{bmatrix}
$$

â¡ï¸ Resultado: manzana = \$5, banana = \$4, zanahoria = \$2

## ğŸ”„ MÃ©todo para encontrar la inversa

1. Se toma la matriz $A$ y se **acompaÃ±a** con la identidad del mismo tamaÃ±o:

$$
\left[ A \mid I \right]
$$

2. Se aplican **operaciones fila** para transformar $A$ en $I$.
3. Al terminar, la parte derecha se habrÃ¡ convertido en $A^{-1}$.

Ejemplo con matriz $3 \times 3$:

$$
\left[\begin{array}{ccc|ccc}
1 & 1 & 3 & 1 & 0 & 0 \\
1 & 2 & 4 & 0 & 1 & 0 \\
1 & 1 & 2 & 0 & 0 & 1
\end{array}\right]
\Rightarrow \cdots \Rightarrow
\left[\begin{array}{ccc|ccc}
1 & 0 & 0 & 1 & -1 & 2 \\
0 & 1 & 0 & -2 & 1 & 1 \\
0 & 0 & 1 & 1 & 0 & -1
\end{array}\right]
$$

La parte derecha es la matriz inversa de $A$.

---
### ğŸ§  Â¿Por quÃ© es Ãºtil encontrar la matriz inversa?

#### 1. **Resolver mÃºltiples sistemas con la misma matriz**

Imagina que tienes una tienda y usas la misma **estructura de productos** (por ejemplo, precios de manzanas, bananas y zanahorias). Cada cliente compra diferentes cantidades, es decir, diferentes vectores $\vec{s}$ (como facturas).

La ecuaciÃ³n es:

$$
 A \cdot \vec{r} = \vec{s}
$$

Donde:

- $A$ representa los precios fijos por producto.
    
- $\vec{r}$ es la cantidad que buscas.
    
- $\vec{s}$ es el total de la compra.
    

Si **ya tienes la inversa $A^{-1}$ calculada**, puedes resolver **cualquier** factura (cualquier $\vec{s}$ nuevo) con solo:

$$
\vec{r} = A^{-1} \cdot \vec{s}
$$
ğŸ” Esto **evita repetir** todo el proceso de eliminaciÃ³n fila por fila para cada nuevo caso. Solo aplicas la fÃ³rmula.

---

#### 2. **Transformaciones de espacio y datos**

Cuando una matriz $A$ actÃºa sobre un vector $\vec{r}$, **lo transforma**: lo estira, rota, refleja o lo traslada dentro del espacio vectorial.

Con la **inversa** $A^{-1}$ puedes:

- Recuperar el vector **original** (deshacer la transformaciÃ³n).
    
- Entender **cÃ³mo cambian los datos** cuando pasas de una representaciÃ³n a otra (por ejemplo, de coordenadas normales a coordenadas de componentes principales en PCA).
    
- Analizar cÃ³mo diferentes entradas afectan salidas, o viceversa.

---

## ğŸ§° Â¿QuÃ© es el determinante?
El **determinante** de una matriz cuadrada mide cuÃ¡nto se **escala el espacio** cuando aplicamos esa matriz como una transformaciÃ³n lineal.

Mide la **escala** de transformaciÃ³n del espacio (Ã¡rea, volumen, etc.)

- Para una matriz $2 \times 2$:
  $$
  A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
  $$
  El determinante se calcula como:
  $$
  \det(A) = ad - bc
  $$

## ğŸ“ InterpretaciÃ³n geomÃ©trica

### ğŸ”² Caso 1: Matriz diagonal (escala directa)
Si:
$$
A = \begin{bmatrix} a & 0 \\ 0 & d \end{bmatrix}
$$

Transforma los vectores base asÃ­:
- $\hat{e}_1 = [1, 0] \rightarrow [a, 0]$
- $\hat{e}_2 = [0, 1] \rightarrow [0, d]$

Esto forma un rectÃ¡ngulo de Ã¡rea $a \cdot d = \det(A)$.

## ğŸ“‰ Â¿QuÃ© ocurre si $\det(A) = 0$?

Cuando:
- Los vectores transformados estÃ¡n alineados (uno es mÃºltiplo del otro)
- El Ã¡rea del paralelogramo es 0
- El espacio se **colapsa a una lÃ­nea**

Entonces:
- La matriz **no tiene inversa**
- Se pierde una dimensiÃ³n
- La informaciÃ³n es irrecuperable
--- 

# âœï¸ ConvenciÃ³n de Sumatoria de Einstein y MultiplicaciÃ³n de Matrices
La **ConvenciÃ³n de Sumatoria de Einstein** es una notaciÃ³n elegante y compacta para expresar operaciones con matrices y vectores, especialmente Ãºtil en programaciÃ³n, Ã¡lgebra lineal y fÃ­sica. Esta notaciÃ³n:

> **Asume una suma sobre cualquier Ã­ndice que aparece repetido en una expresiÃ³n.**  
> No se necesita escribir el sÃ­mbolo âˆ‘ explÃ­citamente.

---

## ğŸ“ MultiplicaciÃ³n de Matrices en NotaciÃ³n de Einstein

Dado que:

- $A$ es una matriz de tamaÃ±o $n \times n$ con elementos $A_{ij}$.
- $B$ es una matriz de tamaÃ±o $n \times n$ con elementos $B_{jk}$.
- Entonces, su producto $C = AB$ tiene elementos:

$$
C_{ik} = \sum_j A_{ij} B_{jk}
$$

Bajo la **convenciÃ³n de Einstein**, se omite el sÃ­mbolo de suma:

$$
C_{ik} = A_{ij} B_{jk}
$$

Se sobreentiende la suma sobre el Ã­ndice **repetido** $j$.


---

## ğŸ”„ Producto Punto como MultiplicaciÃ³n de Matrices

Dado dos vectores columna $u_i$ y $v_i$, su **producto punto** es:

$$
u \cdot v = \sum_i u_i v_i = u_i v_i
$$

Este producto es **equivalente** a una multiplicaciÃ³n de matrices:

$$
u^\top v = u_i v_i
$$

---

## ğŸ“Š ProyecciÃ³n y SimetrÃ­a del Producto Punto

SupÃ³n que $\hat{u}$ es un vector unitario con componentes $u_1, u_2$, y los vectores base canÃ³nicos son:

$$
\hat{e}_1 = \begin{bmatrix}1 \\ 0\end{bmatrix}, \quad \hat{e}_2 = \begin{bmatrix}0 \\ 1\end{bmatrix}
$$

### ProyecciÃ³n de $\hat{u}$ sobre $\hat{e}_1$:

La proyecciÃ³n es simplemente $u_1$.

### ProyecciÃ³n de $\hat{e}_1$ sobre $\hat{u}$:

GeomÃ©tricamente, se obtiene una proyecciÃ³n **idÃ©ntica** en magnitud. Esto refleja que:

$$
\hat{u} \cdot \hat{e}_1 = \hat{e}_1 \cdot \hat{u}
$$

> El producto punto es **simÃ©trico**, y la proyecciÃ³n tambiÃ©n.

---

## ğŸ§© MultiplicaciÃ³n de Matrices No Cuadradas

Se puede multiplicar una matriz $A$ de $m \times n$ por otra $B$ de $n \times k$:
Debe tener mismo numero de columnas matriz $A$ que de filas la matriz $B$

- $A_{ij}$ con $i = 1,\dots,m$, $j = 1,\dots,n$
- $B_{jk}$ con $j = 1,\dots,n$, $k = 1,\dots,k$

Producto resultante $C_{ik}$:

$$
C_{ik} = A_{ij} B_{jk}
$$

**Resultado:** matriz de tamaÃ±o $m \times k$

---

# ğŸ”„ Cambio de Base y Transformaciones con Matrices

## ğŸ§­ **Definiciones Clave**

- Las **columnas de una matriz de transformaciÃ³n** representan los **vectores base** del nuevo sistema en coordenadas del sistema original.
- Transformar un vector de un sistema a otro requiere **multiplicaciÃ³n matricial**.

--- 
### ğŸ§± 1. **Columnas de una matriz de transformaciÃ³n = Vectores base del nuevo sistema**

Imagina que tienes dos mundos:

- ğŸŒ Tu mundo: usa los ejes estÃ¡ndar (horizontal = x, vertical = y).
    
- ğŸ¼ Mundo de Panda: usa ejes inclinados o deformados (por ejemplo, 3 a la derecha y 1 arriba).
    
Cuando quieres representar el mundo de Panda en tu sistema, **necesitas saber cÃ³mo se ven los ejes de Panda en tu mundo**. Y eso se logra colocando sus vectores base como **columnas** en una **matriz de transformaciÃ³n**:
## ğŸ§­ Ejemplo Conceptual

- En el mundo de Panda, sus ejes son:

$$
\hat{e}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
\hat{e}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}
$$

- Pero en tu mundo, esos ejes **no se ven igual**. TÃº ves los vectores base de Panda asÃ­ (  matriz de transformaciÃ³n):

$$
B = \begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}
$$

Esto significa:

- El eje 1 de Panda se ve como $(3, 1)$ en tu sistema.
- El eje 2 de Panda se ve como $(1, 1)$ en tu sistema.

> Cada **columna** = un **vector base de Panda** en tu sistema.
> **B es cÃ³mo se ve el sistema de Panda desde tu sistema.**


---
## ğŸ” Â¿QuÃ© hacen B y Bâ»Â¹?

### **1. Para convertir de la base de Panda â†’ a tu sistema:**

Multiplicas por **B**:

$$
\text{Vector en tu sistema} = B \cdot \text{Vector en base de Panda}
$$

> Traduces un vector **expresado en la base de Panda** a **tu sistema estÃ¡ndar**.

#### ğŸ¼ Ejemplo:
Supongamos que Panda usa los siguientes vectores base (escritos en tu sistema):

$$
B = \begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}
$$

Y en su sistema, el vector es:

$$
\vec{v}_\text{Panda} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

Entonces, en tu sistema, ese vector se ve como:

$$
\vec{v}_\text{tuyo} = B \cdot \vec{v}_\text{Panda} = 
\begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 3 \\ 1 \end{bmatrix}
$$

---

### **2. Para convertir de tu sistema â†’ a la base de Panda:**

Multiplicas por **la inversa de B**:

$$
\text{Vector en base de Panda} = B^{-1} \cdot \text{Vector en tu sistema}
$$

> EstÃ¡s expresando un vector **de tu sistema** en **tÃ©rminos de la base de Panda**.

#### ğŸ¼ Ejemplo:
Tomamos el mismo vector en tu sistema:

$$
\vec{v}_\text{tuyo} = \begin{bmatrix} 3 \\ 1 \end{bmatrix}
$$

La inversa de $B$ es:

$$
B^{-1} = \frac{1}{2}
\begin{bmatrix} 1 & -1 \\ -1 & 3 \end{bmatrix}
$$

Entonces:

$$
\vec{v}_\text{Panda} = B^{-1} \cdot \vec{v}_\text{tuyo} =
\frac{1}{2}
\begin{bmatrix} 1 & -1 \\ -1 & 3 \end{bmatrix}
\begin{bmatrix} 3 \\ 1 \end{bmatrix}
=
\frac{1}{2}
\begin{bmatrix} 2 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

Y asÃ­ comprobamos que el vector $(3, 1)$ en tu sistema es realmente el vector $(1, 0)$ en la base de Panda.

---
<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-19.png" alt="Mi Imagen" width="600">
</div>
## ğŸŸ§ Base Ortogonal de Panda

Cuando la base es **ortonormal** (longitud 1 y ortogonalidad), la transformaciÃ³n inversa es mÃ¡s simple:

Una **base ortonormal** es un conjunto de vectores que cumplen **dos condiciones clave**:

1. **Ortogonalidad** â†’ todos los vectores son **perpendiculares** entre sÃ­.
    
2. **Norma unitaria** â†’ todos los vectores tienen **longitud 1**.
    
Visualmente, son como tus ejes clÃ¡sicos $x$ y $y$, pero **rotados o reflejados**.

- Base de Panda ortonormal:
  $$
  B = \frac{1}{\sqrt{2}}
  \begin{bmatrix}
  1 & -1 \\
  1 & 1
  \end{bmatrix}
  $$


En general, para transformar de tu sistema al sistema de Panda necesitas:

$$
\vec{v}_\text{Panda} = B^{-1} \cdot \vec{v}_\text{tuyo}
$$

Pero como **B es ortonormal**, entonces:

$$
B^{-1} = B^T
$$

Es decir, Â¡la inversa es simplemente la **transpuesta**!

**La transpuesta de una matriz $B$ se obtiene intercambiando filas por columnas.**

$$
B = \frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}
$$
Entonces su transpuesta (e inversa) es:

$$
B^T = \frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
$$


---

## ğŸ“ Proyecciones como TransformaciÃ³n

<div style="text-align: center;">
  <img src="999. IMG FOLDER/proyeccion_ortonormal-1.png" alt="Mi Imagen" width="400">
</div>

Cuando los vectores base forman una **base ortonormal**, podemos evitar el uso de matrices para cambiar de base. En su lugar, usamos **producto punto**.

Para una base ortonormal, se puede evitar usar matrices:
	
* Para cada vector base ortonormal $\hat{b}_i$, calculamos cuÃ¡nto del vector $\vec{v}$ "apunta" en esa direcciÃ³n usando el producto punto:
  $$
  c_i = \vec{v} \cdot \hat{b}_i
  $$

- Vector en nueva base:
  $$
  \vec{v}_\text{nueva} = [c_1, c_2, ..., c_n]
  $$

> âš ï¸ Solo funciona si la base es ortonormal. No funciona si los vectores base no son ortogonales.

### ğŸ§  Ejemplo numÃ©rico

SupÃ³n que:

$$
\vec{v} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}, \quad
\hat{b}_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad
\hat{b}_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} -1 \\ 1 \end{bmatrix}
$$

Entonces:

- $c_1 = \vec{v} \cdot \hat{b}_1 = \frac{1}{\sqrt{2}}(1 + 3) = 2\sqrt{2}$
- $c_2 = \vec{v} \cdot \hat{b}_2 = \frac{1}{\sqrt{2}}(-1 + 3) = \sqrt{2}$

Vector en la nueva base:

$$
\vec{v}_\text{nueva} = \begin{bmatrix} 2\sqrt{2} \\ \sqrt{2} \end{bmatrix}
$$

---

## ğŸŒ€ Transformaciones dentro de Bases No Ortogonales

Digamos que:
- Tienes una base "rara" como la de Panda (no ortonormal, por ejemplo $B = \begin{bmatrix} 3 & 1 \ 1 & 1 \end{bmatrix}$).
    
- Quieres aplicar una transformaciÃ³n, por ejemplo, una **rotaciÃ³n de 45Â°**.
    
- Pero... **esa rotaciÃ³n estÃ¡ escrita en tu base estÃ¡ndar (la ortonormal)**.
    
Entonces surge el problema:

> â“ Â¿CÃ³mo aplicar una transformaciÃ³n escrita en tu mundo al vector que vive en el mundo de Panda?

### ğŸ§  La soluciÃ³n en 3 pasos

#### ğŸ”¹ 1. Convierte el vector de Panda a tu sistema

Este paso te permite **interpretar el vector de Panda usando tus coordenadas**:

$$
\vec{v} ' = B \cdot \vec{v}_\text{Panda}
$$

AquÃ­ $\vec{v}_\text{Panda}$ es el vector descrito con los ejes de Panda, y $B$ lo traduce a tu mundo.

---

#### ğŸ”¹ 2. Aplica la transformaciÃ³n (por ejemplo, rotaciÃ³n $R$)

Ahora que tienes el vector en tu sistema, aplicas la transformaciÃ³n normalmente:

$$
R \cdot \vec{v}'
$$

Por ejemplo, una rotaciÃ³n de 45Â° en tu base es:

$$
R = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & -1 \\ 1 & 1 \end{bmatrix}
$$

---

#### ğŸ”¹ 3. Regresa el resultado al mundo de Panda

Una vez que el vector ha sido transformado, lo conviertes de vuelta a la base de Panda:

$$
\vec{v}_\text{rotada} = B^{-1} \cdot (R \cdot \vec{v}')
$$

Esto expresa el vector final **en tÃ©rminos de la base de Panda**.


> Esto se resume como:

$$
\vec{v}_\text{rotada} = B^{-1} R B \cdot \vec{v}_\text{Panda}
$$
### ğŸ“Œ IntuiciÃ³n

- $B$ lleva **de Panda a ti**  
- $R$ aplica la transformaciÃ³n **en tu mundo**  
- $B^{-1}$ lleva **de vuelta a Panda**

Panda no sabe quÃ© es una rotaciÃ³n de 45Â° porque su base no es ortonormal.  
TÃº haces la rotaciÃ³n en tu lenguaje y luego se la traduces de regreso.

---
## Ejemplo: TransformaciÃ³n en Base No Ortonormal

### ğŸ¼ Base de Panda (no ortonormal):

$$
B = \begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix}
$$
 Vector en la base de Panda:

$$
\vec{v}_\text{Panda} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}
$$
### ğŸ” Paso 1: Llevar el vector al sistema estÃ¡ndar

Multiplicamos por $B$:

$$
\vec{v}' = B \cdot \vec{v}_\text{Panda} = \begin{bmatrix} 3 & 1 \\ 1 & 1 \end{bmatrix} \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 7 \\ 3 \end{bmatrix}
$$


### ğŸ”„ Paso 2: Aplicar una rotaciÃ³n de 45Â° en el sistema estÃ¡ndar

Matriz de rotaciÃ³n:

$$
R = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & -1 \\ 1 & 1 \end{bmatrix}
$$

Aplicamos la rotaciÃ³n:

$$
\vec{v}_\text{rotado}' = R \cdot \vec{v}' = \begin{bmatrix} 2.8284 \\ 7.0711 \end{bmatrix}
$$

### ğŸ”ƒ Paso 3: Regresar a la base de Panda

Inversa de $B$:

$$
B^{-1} = \begin{bmatrix} 1 & -1 \\ -1 & 3 \end{bmatrix} \cdot \frac{1}{2}
$$

Aplicamos la transformaciÃ³n inversa:

$$
\vec{v}_\text{rotada} = B^{-1} \cdot \vec{v}_\text{rotado}' = \begin{bmatrix} 2.0 \\ 1.0 \end{bmatrix}
$$
Transpuesta, Inversa y Matrices Ortogonales

### ğŸ”„ Â¿QuÃ© es la transpuesta?

La **transpuesta** de una matriz $A$ es una matriz $A^T$ donde se **intercambian filas por columnas**. Formalmente:

$$
(A^T)_{ij} = A_{ji}
$$

Ejemplo:

$$
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} 
\Rightarrow 
A^T = \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix}
$$

---
## ğŸ” Transpuesta  Matrices Ortogonales

### ğŸ”„ Â¿QuÃ© es la transpuesta?

La **transpuesta** de una matriz $A$ es una matriz $A^T$ donde se **intercambian filas por columnas**. Formalmente:

$$
(A^T)_{ij} = A_{ji}
$$

Ejemplo:

$$
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} 
\Rightarrow 
A^T = \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix}
$$

---

### ğŸ§± Matrices de cambio de base con vectores ortonormales

Una matriz de cambio de base $A$ cuyos vectores columna son **ortonormales** (perpendiculares y de longitud 1) cumple:

- $a_i \cdot a_j = 0$ si $i \ne j$ (ortogonalidad)
- $a_i \cdot a_i = 1$ (longitud unitaria)

Entonces:

$$
A^T A = I
$$

Â¡Esto significa que la transpuesta de $A$ es su inversa!:

$$
A^{-1} = A^T
$$

---

### ğŸ“ Â¿QuÃ© es una matriz ortogonal?

Una matriz **ortogonal** es una matriz cuadrada cuyas columnas (o filas) son un conjunto ortonormal. Cumple:

$$
A^T = A^{-1}
$$

AdemÃ¡s:

- Su determinante es $\pm 1$
- No deforma el espacio (preserva distancias y Ã¡ngulos)

---

### ğŸ¯ Aplicaciones en ciencia de datos

Usar **matrices ortogonales** para transformar datos:

- Permite revertir fÃ¡cilmente la transformaciÃ³n
- No colapsa el espacio
- El cambio de base se convierte en una simple proyecciÃ³n (producto punto)
- Es ideal en algoritmos como **PCA**

---
## ğŸ§  Proceso de Gram-Schmidt

El proceso de Gram-Schmidt no es solo un algoritmo matemÃ¡tico, sino una herramienta con propÃ³sito claro: transformar un conjunto de vectores **linealmente independientes** (pero desordenados) en una base **ortonormal**, es decir:

- Vectores **perpendiculares** entre sÃ­
- De **longitud unitaria**
- Que abarcan el mismo espacio que los vectores originales

---

### ğŸ¯ Â¿QuÃ© problema resuelve?

SupÃ³n que tienes vectores inclinados, largos o cortos, que no estÃ¡n a 90Â° entre sÃ­. Trabajar con ellos puede ser complejo:

| SituaciÃ³n                             | Problemas con vectores arbitrarios | Con base ortonormal |
|--------------------------------------|------------------------------------|---------------------|
| Proyecciones                         | DifÃ­ciles sin ajustes              | Basta con producto punto |
| Cambios de base                      | Requiere invertir matriz           | Solo se transpone |
| Rotaciones/transformaciones          | CÃ¡lculo complejo                   | Ãlgebra limpia |
| AnÃ¡lisis de datos (PCA, etc.)        | Datos mezclados, no separables     | Componentes limpios |

---

### ğŸ§  IntuiciÃ³n geomÃ©trica

Cada paso de Gram-Schmidt:

1. **Toma un vector nuevo**
2. **Le quita** lo que ya estaba contenido en los anteriores
3. **Normaliza** lo que queda

AsÃ­ conseguimos vectores **mutuamente perpendiculares**, que **no se repiten entre sÃ­** y **forman una base mÃ¡s "inteligible"** del espacio.

---

## ğŸ§ª Proceso de Gram-Schmidt paso a paso

Dado un conjunto de vectores $\vec{v}_1, \vec{v}_2, ..., \vec{v}_n$ linealmente independientes:

1. **Primer vector**:
   $$
   \vec{e}_1 = \frac{\vec{v}_1}{\|\vec{v}_1\|}
   $$

2. **Segundo vector**:
   $$
   \vec{u}_2 = \vec{v}_2 - (\vec{v}_2 \cdot \vec{e}_1)\vec{e}_1
   $$
   $$
   \vec{e}_2 = \frac{\vec{u}_2}{\|\vec{u}_2\|}
   $$

3. **Tercer vector**:
   $$
   \vec{u}_3 = \vec{v}_3 - (\vec{v}_3 \cdot \vec{e}_1)\vec{e}_1 - (\vec{v}_3 \cdot \vec{e}_2)\vec{e}_2
   $$
   $$
   \vec{e}_3 = \frac{\vec{u}_3}{\|\vec{u}_3\|}
   $$

...y asÃ­ sucesivamente hasta obtener una base ortonormal $\{\vec{e}_1, \vec{e}_2, ..., \vec{e}_n\}$

---

## ğŸ“Œ Resultado final

- Los vectores $\vec{e}_i$ son ortonormales.
- Siguen abarcando el mismo espacio que los $\vec{v}_i$.
- Se pueden usar para simplificar transformaciones, proyecciones y anÃ¡lisis.

---
# ğŸ§® IntroducciÃ³n Visual a Eigenvectores y Eigenvalores 

## ğŸ—£ï¸ Â¿QuÃ© significa "Eigen"?

La palabra **"eigen"** proviene del alemÃ¡n y significa **caracterÃ­stico**. AsÃ­ que cuando hablamos de un **problema de autovalores/autovectores**, nos referimos a encontrar las **propiedades caracterÃ­sticas de una transformaciÃ³n lineal**.

---

## ğŸ”„ Transformaciones Lineales

Las matrices pueden representar transformaciones como:

- Escalado
- RotaciÃ³n
- Cizalladura (shear)

Imagina aplicar una de estas transformaciones a todos los vectores del plano. Un buen truco visual es dibujar un **cuadrado centrado en el origen** y ver cÃ³mo se deforma.

---
<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-20.png" alt="Mi Imagen" width="500">
</div>
## ğŸ¨ Ejemplo: Escalado vertical


Supongamos un escalado vertical por un factor de 2:

- El cuadrado original se convierte en un **rectÃ¡ngulo mÃ¡s alto**.
- Dibujamos tres vectores: uno horizontal, uno vertical, y uno diagonal.

**Resultado:**

- El vector **horizontal** no cambia.
- El vector **vertical** se duplica en longitud.
- El vector **diagonal** cambia de direcciÃ³n y longitud.

ğŸ‘‰ Los vectores **que permanecen en su misma lÃ­nea de acciÃ³n** se llaman **autovectores**.

- Si su longitud cambia por un factor $\lambda$, ese nÃºmero se llama **autovalor**.

---

## ğŸ“Œ Definiciones Clave

- **Autovector**: Vector que **mantiene su direcciÃ³n** bajo una transformaciÃ³n.
- **Autovalor**: Escala por la que el autovector es **alargado o contraÃ­do**.

---

# ğŸ“ FormulaciÃ³n Formal de eigenvectors y eigenvalues

## ğŸ§  Recordatorio Conceptual

- Un **autovector** de una matriz $A$ es un vector que **no cambia de direcciÃ³n** al aplicar la transformaciÃ³n lineal $A$.
- Un **autovalor** es la **escala** por la cual se multiplica ese autovector.

---

## ğŸ§¾ EcuaciÃ³n CaracterÃ­stica

La expresiÃ³n algebraica clave es:

$$
A \vec{x} = \lambda \vec{x}
$$

- $A$ es una matriz cuadrada $n \times n$.
- $\vec{x}$ es el autovector.
- $\lambda$ es el autovalor.

Esta ecuaciÃ³n dice: aplicar $A$ a $\vec{x}$ es lo mismo que escalar $\vec{x}$ por $\lambda$.

---

## ğŸ”„ Reescritura para resolver

Llevamos todo a un lado:

$$
A\vec{x} - \lambda I \vec{x} = 0
$$

Factorizamos:

$$
(A - \lambda I) \vec{x} = 0
$$

- $I$ es la matriz identidad del mismo tamaÃ±o que $A$.

---

## â— CondiciÃ³n para soluciones no triviales

Queremos **evitar** $\vec{x} = 0$ (soluciÃ³n trivial).

Entonces:

$$
\det(A - \lambda I) = 0
$$

Esto es la **ecuaciÃ³n caracterÃ­stica** que nos da los **autovalores**.

---

## ğŸ§ª Ejemplo: Escalado vertical

Sea:

$$
A = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}
$$

Entonces:

$$
\det(A - \lambda I) = 
\det \begin{bmatrix}
1 - \lambda & 0 \\ 
0 & 2 - \lambda
\end{bmatrix}
= (1 - \lambda)(2 - \lambda)
$$

Entonces los autovalores son:

$$
\lambda = 1, \quad \lambda = 2
$$


### âœ… Para $\lambda = 1$:

**1. Calculamos:**

$$
A - \lambda I =
\begin{bmatrix}
1 & 0 \\
0 & 2
\end{bmatrix}
-
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}
$$

**2. Sistema resultante:**

$$
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

Ecuaciones:

- $0x_1 + 0x_2 = 0$ (trivial)
- $0x_1 + 1x_2 = 0 \Rightarrow x_2 = 0$

**3. Autovector asociado:**

$$
\vec{x}_1 =
\begin{bmatrix}
t \\
0
\end{bmatrix}
\quad t \in \mathbb{R}
$$


### âœ… Para $\lambda = 2$:

**1. Calculamos:**

$$
A - \lambda I =
\begin{bmatrix}
1 & 0 \\
0 & 2
\end{bmatrix}
-
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix}
=
\begin{bmatrix}
-1 & 0 \\
0 & 0
\end{bmatrix}
$$

**2. Sistema resultante:**

$$
\begin{bmatrix}
-1 & 0 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

Ecuaciones:

- $-x_1 = 0 \Rightarrow x_1 = 0$
- $0x_2 = 0$ (trivial)

**3. Autovector asociado:**

$$
\vec{x}_2 =
\begin{bmatrix}
0 \\
t
\end{bmatrix}
\quad t \in \mathbb{R}
$$

---

### ğŸ§  Nota sobre el parÃ¡metro $t$:

Usamos $t$ como **parÃ¡metro libre** porque cualquier mÃºltiplo escalar de un autovector tambiÃ©n es un autovector. Lo que importa es su **direcciÃ³n**, no su magnitud.

---

## ğŸš« Caso sin autovectores reales: RotaciÃ³n 90Â°

Sea:

$$
A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
$$

Entonces:

$$
\det(A - \lambda I) = 
\det \begin{bmatrix}
-\lambda & -1 \\ 
1 & -\lambda
\end{bmatrix}
= \lambda^2 + 1 = 0
$$

Esto no tiene soluciones reales â†’ âŒ no hay autovectores reales.

---

## ğŸ“Œ ConclusiÃ³n

- La ecuaciÃ³n $A\vec{x} = \lambda\vec{x}$ describe quÃ© vectores mantienen su direcciÃ³n tras una transformaciÃ³n.
- La ecuaciÃ³n caracterÃ­stica $\det(A - \lambda I) = 0$ nos da los autovalores.
- Luego se encuentran los autovectores resolviendo $(A - \lambda I)\vec{x} = 0$.

---

## âœ¨ DiagonalizaciÃ³n y Potencias de Matrices

### ğŸ” MotivaciÃ³n: MÃºltiples Transformaciones
Cuando una transformaciÃ³n lineal $T$ se aplica muchas veces, calcular $T^n$ directamente puede ser computacionalmente costoso.

Ejemplo:
- $\vec{v}_1 = T\vec{v}_0$
- $\vec{v}_2 = T\vec{v}_1 = T^2 \vec{v}_0$
- $\vec{v}_n = T^n \vec{v}_0$

Si $n = 1{,}209{,}600$ (dos semanas en segundos), entonces $T^n$ es muy costoso de calcular directamente.

---

### ğŸ“ DiagonalizaciÃ³n: El Truco del Cambio de Base
La soluciÃ³n estÃ¡ en cambiar de base a la **base de autovectores** (eigen-basis), donde $T$ se convierte en una **matriz diagonal** $D$:

$$
T = C D C^{-1}
$$

Entonces:

$$
T^n = C D^n C^{-1}
$$

Esto permite calcular $T^n$ fÃ¡cilmente, porque:

- $D$ es diagonal â‡’ $D^n$ se obtiene elevando cada tÃ©rmino en la diagonal.
- $C$ contiene los autovectores como columnas.
- $D$ contiene los autovalores correspondientes en su diagonal.

---

### ğŸ§ª Ejemplo con $T = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}$

Dada la matriz de transformaciÃ³n:

$$
T = \begin{bmatrix}
1 & 1 \\
0 & 2
\end{bmatrix}
$$
### Paso 1: CÃ¡lculo de autovalores

Resolvemos el polinomio caracterÃ­stico:

$$
\det(T - \lambda I) = 
\begin{vmatrix}
1 - \lambda & 1 \\
0 & 2 - \lambda
\end{vmatrix}
= (1 - \lambda)(2 - \lambda) = 0
$$

Autovalores:
- $$\lambda_1 = 1$$
- $$\lambda_2 = 2$$


### Paso 2: CÃ¡lculo de eigenvectores

### Para $$\lambda = 1$$

Sustituimos en $( (T - \lambda I) \vec{x} = 0 )$:

$$
\begin{bmatrix}
0 & 1 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
= 0 \Rightarrow x_2 = 0
$$

Autovector asociado:

$$
\vec{x}_1 = \begin{bmatrix}
t \\
0
\end{bmatrix}
\Rightarrow \text{Elegimos } t = 1 \Rightarrow \vec{x}_1 = \begin{bmatrix}
1 \\
0
\end{bmatrix}
$$

### Para $$\lambda = 2$$

$$
\begin{bmatrix}
-1 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
= 0 \Rightarrow -x_1 + x_2 = 0 \Rightarrow x_1 = x_2
$$

Autovector asociado:

$$
\vec{x}_2 = \begin{bmatrix}
t \\
t
\end{bmatrix}
\Rightarrow \text{Elegimos } t = 1 \Rightarrow \vec{x}_2 = \begin{bmatrix}
1 \\
1
\end{bmatrix}
$$


### Paso 3: Matriz de cambio de base

Construimos la matriz \( C \) con los eigenvectores  como columnas:

$$
C = \begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
$$

### Paso 4: Inversa de \( C \)

Como  $\det(C) = 1$, usamos la fÃ³rmula para $( 2 \times 2$):

$$
C^{-1} = \begin{bmatrix}
1 & -1 \\
0 & 1
\end{bmatrix}
$$


Matriz diagonal $D$:

$$
D = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}
$$

Entonces:

$$
T^2 = C D^2 C^{-1} =
\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix}
\begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix} =
\begin{bmatrix} 1 & 3 \\ 0 & 4 \end{bmatrix}
$$

Aplicando $T^2$ a $\vec{v} = \begin{bmatrix}-1 \\ 1\end{bmatrix}$:

$$
T^2\vec{v} = \begin{bmatrix}1 & 3 \\ 0 & 4\end{bmatrix} \begin{bmatrix}-1 \\ 1\end{bmatrix} = \begin{bmatrix}2 \\ 4\end{bmatrix}
$$

Â¡Mismo resultado que con multiplicaciones directas!

---

### ğŸ’¡ ConclusiÃ³n
Diagonalizar $T$ simplifica el cÃ¡lculo de $T^n$. Si entiendes:

- Autovalores
- Autovectores
- Cambio de base

...entonces puedes explotar esta poderosa tÃ©cnica en Ã¡lgebra lineal y machine learning.

> Esta tÃ©cnica es especialmente Ãºtil en dinÃ¡mica de sistemas, Markov Chains, simulaciones fÃ­sicas, etc.
