
#Machine_Learning
#Unsupervised_Learning
# ğŸ“Š K-Means Clustering

## ğŸ“Œ What is Clustering?

**Clustering** is an **unsupervised learning** technique used to group data points so that:
- Samples in the **same group (cluster)** are **similar**
- Samples in **different groups** are **dissimilar**

### Key Questions
1. **How many clusters (`k`) should we choose?**
2. **How do we measure similarity between data points?**

---

## ğŸš€ What is K-Means?

**K-Means** is one of the most widely used clustering algorithms.

- **k**: Number of clusters
- **Means**: Refers to the **centroids**, or the average location of data points in each cluster

### ğŸ§  K-Means Algorithm (Iterative Process)

1. Randomly initialize **k centroids**
2. Assign each data point to its **nearest centroid**
3. Recalculate centroids as the **mean** of assigned points
4. Repeat steps 2â€“3 until **convergence**

> âœ… **Convergence** means: data points no longer switch clusters, and centroids stabilize.

---

## ğŸ”„ Training vs Inference

- **Training**: The iterative process to find the best cluster centroids.
- **Inference**: After training, we can assign new data points to the closest cluster **without retraining**.

---

## ğŸ“‰ Choosing the Right Number of Clusters

The performance of K-Means heavily depends on the value of **k**.

### âš™ï¸ Inertia

- **Inertia**: Sum of squared distances from each point to its assigned clusterâ€™s centroid.
- Lower inertia means **tighter clusters**.

```python
print(model.inertia_)
```

### ğŸ“ˆ The Elbow Method

- Plot **inertia vs k**.
- Look for a point where the rate of decrease sharply slows down â†’ the "**elbow**".
- This value of `k` is often a good trade-off between **model complexity** and **tight clustering**.

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertias = []
ks = range(1, 10)

for k in ks:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X)
    inertias.append(model.inertia_)

plt.plot(ks, inertias, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")
plt.title("Elbow Method for Optimal k")
plt.show()
```

> ğŸ“Œ In the Iris dataset, the elbow usually appears at `k = 3`, which matches the number of Iris species.

---

## ğŸ“Š Visualizing K-Means

After fitting the model, we can visualize how the data was grouped:

```python
import seaborn as sns

model = KMeans(n_clusters=3, random_state=42)
model.fit(X)

labels = model.labels_

sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette="Set2")
```

> ğŸ¯ Use only the first two features if your dataset has more than two dimensions, or apply dimensionality reduction (e.g., PCA).
