#Machine_Learning


# ğŸ”¢ Transformaciones NumÃ©ricas 

## 1. ğŸ¯ Centering (Centrado)

### Â¿QuÃ© es?
Resta la media a cada valor, haciendo que los datos tengan media cero.

$$
x_{centered} = x - \bar{x}
$$

### Â¿CuÃ¡ndo usarlo?
- Cuando los modelos son sensibles a la ubicaciÃ³n de los datos (e.g. PCA, regresiÃ³n lineal).
- Como paso previo a otras transformaciones como escalado o PCA.

### Â¿CÃ³mo en Python?

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])
x_centered = x - np.mean(x)
```

---

## 2. âš–ï¸ Standard Scaler (EstandarizaciÃ³n)

### Â¿QuÃ© es?
Transforma los datos para que tengan **media 0 y desviaciÃ³n estÃ¡ndar 1**.

$$
x_{std} = \frac{x - \mu}{\sigma}
$$

### Â¿CuÃ¡ndo usarlo?
- Modelos que asumen datos normalizados: regresiÃ³n logÃ­stica, SVM, PCA.
- Antes de aplicar algoritmos de distancia (KNN, clustering).

### Â¿CÃ³mo en Python?

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x.reshape(-1, 1))
```

---

## 3. ğŸ“Š Min-Max Scaler

### Â¿QuÃ© es?
Escala los datos a un rango definido, usualmente entre 0 y 1.

$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

### Â¿CuÃ¡ndo usarlo?
- Cuando se requiere que los datos estÃ©n en un rango fijo (e.g. redes neuronales).
- Para interpretaciones comparables entre variables.

### Â¿CÃ³mo en Python?

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x_minmax = scaler.fit_transform(x.reshape(-1, 1))
```

---

## 4. ğŸ§± Binning (DiscretizaciÃ³n)

### Â¿QuÃ© es?
Divide los datos en intervalos o "cubetas" (bins), convirtiendo datos continuos en categorÃ­as.

### Â¿CuÃ¡ndo usarlo?
- Cuando se quiere reducir ruido o modelar no linealidades.
- Ãštil para Ã¡rboles de decisiÃ³n y reglas.

### Â¿CÃ³mo en Python?

```python
import pandas as pd

x_binned = pd.cut(x, bins=3, labels=["bajo", "medio", "alto"])
```

---

## 5. ğŸ” Transformaciones LogarÃ­tmicas

### Â¿QuÃ© es?
Aplica una funciÃ³n logarÃ­tmica a los datos.

$$
x' = \log(x + 1)
$$

Se suele usar log(x + 1) para evitar problemas con ceros.

### Â¿CuÃ¡ndo usarlo?
- Cuando hay **asimetrÃ­a positiva** (distribuciones con colas largas a la derecha).
- Para estabilizar la varianza.

### Â¿CÃ³mo en Python?

```python
import numpy as np

x_log = np.log1p(x)  # log(x + 1)
```

---

## âœ… ConclusiÃ³n

| TransformaciÃ³n      | Â¿CuÃ¡ndo usarla?                                |
|---------------------|------------------------------------------------|
| Centering           | Como paso previo a PCA o modelos lineales      |
| Standard Scaler     | Para modelos sensibles a escalas o distancias  |
| Min-Max Scaler      | Cuando se necesita un rango fijo (e.g. [0, 1]) |
| Binning             | Para reducir complejidad o modelar categorÃ­as  |
| Log Transformation  | Para corregir asimetrÃ­a o escalar colas largas |

> ğŸ” *El preprocesamiento numÃ©rico adecuado es clave para un buen desempeÃ±o del modelo.*
