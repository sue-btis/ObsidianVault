#Machine_Learning 
[[3.3 Linear Regression OLS vs Gradient Descent]]

# 📈 Multiple Linear Regression 

La **regresión lineal múltiple** es una extensión de la regresión lineal simple donde se utilizan **dos o más variables independientes** para predecir una variable dependiente continua.

---

## 🧠 Idea Principal

En lugar de usar una sola entrada $x$, ahora usamos múltiples características $x_1, x_2, ..., x_n$:

$$\hat{y} = b + m_1x_1 + m_2x_2 + \dots + m_nx_n$$

- $\hat{y}$: predicción del valor de salida
- $b$: intercepto
- $m_i$: coeficientes que indican cuánto influye cada variable $x_i$ en $y$

---

## 🔎 ¿Cuándo usar regresión múltiple?

Cuando el fenómeno que quieres predecir depende de múltiples factores.  
Por ejemplo: predecir el precio de una casa según su superficie, número de habitaciones y ubicación.

---

## 📊 Correlación

Antes de entrenar un modelo, conviene revisar si las variables están correlacionadas con la salida:

- **Correlación positiva**: $r$ cerca de +1  
- **Correlación negativa**: $r$ cerca de -1  
- **Sin correlación**: $r \approx 0$

Coeficiente de correlación de Pearson:

$$r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}$$

Esto ayuda a decidir qué variables incluir en el modelo.

---

## 💻 Código en Python

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import matplotlib.pyplot as plt

# Dataset de ejemplo
data = pd.DataFrame({
    "horas_estudio": [1, 2, 3, 4, 5],
    "horas_sueño": [8, 7, 6, 5, 4],
    "nota": [60, 65, 70, 75, 80]
})

# Variables independientes y dependiente
X = data[["horas_estudio", "horas_sueño"]]
y = data["nota"]

# Modelo
model = LinearRegression()
model.fit(X, y)

# Predicción
y_pred = model.predict(X)

# Resultados
print("Coeficientes:", model.coef_)
print("Intercepto:", model.intercept_)

#Evaluacion del Modelo
print("R²:", r2_score(y, y_pred))
print("MSE:", mean_squared_error(y, y_pred))

# Correlación
print(data.corr())

# Gráfico de correlación
sns.heatmap(data.corr(), annot=True, cmap="coolwarm")
plt.title("Matriz de Correlación")
plt.show()
```

---

## 📐 Evaluación del Modelo

### ✅ R² (Coeficiente de determinación)

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

- Mide qué tan bien el modelo explica la variabilidad de los datos.
- Valor ideal: cercano a 1.  
- Si $R^2 = 0.9$, el modelo explica el 90% de la variación.

---

### ✅ MSE (Mean Squared Error)

$$MSE = \frac{1}{n} \sum (y_i - \hat{y}_i)^2$$

- Valor ideal: cercano a 0.
- Útil para comparar modelos.

---

## 🚨 Cuidado con:

- **Colinealidad**: cuando dos variables explican lo mismo, pueden distorsionar los coeficientes.
- **Sobreajuste (overfitting)**: cuando tu modelo es muy preciso en los datos de entrenamiento pero falla con nuevos datos.
- **Datos categóricos**: se deben convertir a variables numéricas (One-Hot Encoding).

---
## 🧠 Recomendaciones

- Usa `corr()` para elegir variables relevantes.
- Visualiza con `sns.heatmap()` o scatter plots.
- Evalúa modelos con múltiples métricas, no solo R².
- Prueba con datos nuevos (validación cruzada si es posible).

