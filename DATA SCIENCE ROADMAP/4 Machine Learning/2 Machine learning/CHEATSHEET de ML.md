
# üß† Machine Learning & Data Science Cheatsheet

Gu√≠a exhaustiva y pedag√≥gica para aprender desde cero a nivel intermedio-avanzado en ML y DS.

---

# üì¶ Parte 1: Exploraci√≥n, Manipulaci√≥n y An√°lisis de Datos con Pandas

```python
import pandas as pd
```

### üß© Estructuras de Datos

```python
# Serie
serie = pd.Series([10, 20, 30], index=['a', 'b', 'c'])

# DataFrame
datos = {'Nombre': ['Ana', 'Luis', 'Juan'], 'Edad': [23, 45, 34]}
df = pd.DataFrame(datos)
```

### üì• Cargar y Crear DataFrames

```python
df = pd.read_csv('archivo.csv')
data = np.array([[1, 2, 3], [4, 5, 6]])
df = pd.DataFrame(data, columns=['A', 'B', 'C'])
```

### üîç Selecci√≥n de Datos

```python
df['A']         # Columna
df.iloc[0]      # Por posici√≥n
df.loc[0, 'A']  # Por etiqueta
```

### ‚ûï Transformaci√≥n y Filtrado

```python
df['D'] = df['A'] + df['B']
df['E'] = df['B'].apply(lambda x: x * 2)
df = df[df['A'] > 2]
df.drop('D', axis=1, inplace=True)
```

### üìä Estad√≠sticas B√°sicas

```python
df.mean(), df.median(), df.std()
df.describe()
df.corr(), df.cov()
```

### üîÑ Agrupamiento y Agregaciones

```python
df.groupby('A').agg({'B': ['mean', 'sum'], 'C': 'max'})
df.agg({'A': ['mean'], 'B': ['min', 'max']})
```

### üìà Pivot Tables

```python
df.pivot_table(values='Sales', index='City', columns='Year', aggfunc='sum')
```

### üîó Merge y Join

```python
pd.merge(df1, df2, on='ID', how='inner')
pd.concat([df1, df2], axis=0, ignore_index=True)
```

---

## Manipulaci√≥n Num√©rica y √Ålgebra Lineal con NumPy

```python
import numpy as np
```

### üßÆ Crear Arrays

```python
v = np.array([1, 2, 3])
A = np.array([[1, 2], [3, 4]])
I = np.eye(3)
Z = np.zeros((2, 3))
```

### üîÑ Transformaciones

```python
v.reshape((3, 1))
A.T  # Transpuesta
```

### üîó Construcci√≥n y Broadcasting

```python
np.column_stack((col1, col2))
v + 5  # Broadcasting
np.sum(v)
```

### üí• Operaciones Elementales

```python
A + B
A * B
np.dot(A, B)
A @ B  # Multiplicaci√≥n matricial
```

### üìê √Ålgebra Lineal

```python
np.linalg.inv(A)
np.linalg.det(A)
eig_vals, eig_vecs = np.linalg.eig(A)
np.linalg.solve(A, b)
```

---

## üîÑ Flujo B√°sico de un Proyecto de Machine Learning

1. **Recolecci√≥n de datos**: desde CSV, APIs o bases de datos.
2. **Limpieza**: manejo de datos faltantes, duplicados, valores extremos.
3. **An√°lisis exploratorio**: `describe()`, `groupby()`, `value_counts()`.
4. **Visualizaci√≥n**: `matplotlib`, `seaborn` para entender la distribuci√≥n.
5. **Preparaci√≥n de datos**: codificaci√≥n, escalado, divisi√≥n train/test.
6. **Entrenamiento del modelo**: scikit-learn (`LinearRegression`, `KNeighborsClassifier`, etc.).
7. **Evaluaci√≥n del modelo**: m√©tricas (accuracy, MSE, F1-score...).
8. **Ajuste y validaci√≥n cruzada**: `GridSearchCV`, regularizaci√≥n.
9. **Despliegue**: exportar con `joblib`, usar en APIs o dashboards.

---

(En la siguiente parte se incluir√°n modelos de ML supervisado y no supervisado, m√©tricas y visualizaci√≥n avanzada.)

---
## üë®‚Äçüè´ Conceptos Clave

### üéØ Variables X (features) e y (target)

- **X**: matriz de caracter√≠sticas (variables independientes)
- **y**: variable objetivo (lo que queremos predecir)

```python
X = df.drop("target", axis=1)
y = df["target"]
```

### ‚ö†Ô∏è Overfitting vs. Underfitting

- **Overfitting**: El modelo aprende demasiado bien los datos de entrenamiento y falla al generalizar.
- **Underfitting**: El modelo no logra captar la relaci√≥n subyacente en los datos.

| Tipo de error    | Causa principal                    | Soluci√≥n com√∫n                  |
|------------------|------------------------------------|----------------------------------|
| Overfitting       | Modelo demasiado complejo          | Regularizaci√≥n, m√°s datos       |
| Underfitting      | Modelo demasiado simple            | Modelo m√°s complejo             |

### ‚öñÔ∏è Bias-Variance Tradeoff

- **Bias**: Error por suposiciones incorrectas del modelo.
- **Variance**: Sensibilidad del modelo a peque√±as variaciones en los datos.

Ideal: bajo bias y baja varianza ‚Üí buen desempe√±o generalizado.

---
# ‚ûïParte 2 : Metodos de Transformaci√≥n
## üî¢1. Transformaciones Num√©ricas y  de Variables categ√≥ricas

### üß© Transformaciones Num√©ricas

#### 1. üéØ Centering (Centrado)
$$
x_{centered} = x - \bar{x}
$$

```python
x_centered = x - np.mean(x)
```

---

#### 2. ‚öñÔ∏è Standard Scaler (Estandarizaci√≥n)
$$
x_{std} = \frac{x - \mu}{\sigma}
$$

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x.reshape(-1, 1))
```

---

#### 3. üìä Min-Max Scaler
$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_minmax = scaler.fit_transform(x.reshape(-1, 1))
```

---

#### 4. üß± Binning (Discretizaci√≥n)

```python
x_binned = pd.cut(x, bins=3, labels=["bajo", "medio", "alto"])
```

---

#### 5. üîÅ Transformaciones Logar√≠tmicas
$$
x' = \log(x + 1)
$$

```python
x_log = np.log1p(x)
```

---

### üß© Transformaciones de Variables Categ√≥ricas

#### üî¢ Ordinal Encoding

```python
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder(categories=[['Excellent', 'New', 'Like New', 'Good', 'Fair']])
encoded = encoder.fit_transform(cars['condition'].values.reshape(-1,1))
```

---

#### üè∑Ô∏è Label Encoding

```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
cars['color'] = encoder.fit_transform(cars['color'])
```

---

#### üü¶ One-Hot Encoding

```python
ohe = pd.get_dummies(cars['color'])
cars = cars.join(ohe)
```

---

#### üî¢ Binary Encoding

```python
from category_encoders import BinaryEncoder
colors = BinaryEncoder(cols=['color']).fit_transform(cars)
```

---

#### üí† Hashing Encoding

```python
from category_encoders import HashingEncoder
encoder = HashingEncoder(cols='color', n_components=5)
hash_results = encoder.fit_transform(cars['color'])
```

---

#### üéØ Target Encoding

```python
from category_encoders import TargetEncoder
encoder = TargetEncoder(cols='color')
encoded = encoder.fit_transform(cars['color'], cars['sellingprice'])
```

---

#### ‚è∞ Encoding de Fechas

```python
cars['saledate'] = pd.to_datetime(cars['saledate'])
cars['month'] = cars['saledate'].dt.month
cars['dayofweek'] = cars['saledate'].dt.dayofweek
cars['yearbuild_sold'] = cars['saledate'].dt.year - cars['year']
```

---

### ‚úÖ Conclusi√≥n

| Transformaci√≥n      | ¬øCu√°ndo usarla?                                |
|---------------------|------------------------------------------------|
| Centering           | PCA o modelos lineales                         |
| Standard Scaler     | Modelos sensibles a escala (SVM, regresi√≥n)    |
| Min-Max Scaler      | Redes neuronales, normalizaci√≥n entre [0, 1]  |
| Binning             | Modelos de reglas, simplificaci√≥n              |
| Log Transform       | Reducci√≥n de asimetr√≠a                         |

| Codificaci√≥n         | ¬øCu√°ndo usarla?                               |
|----------------------|-----------------------------------------------|
| Ordinal              | Categor√≠as ordenadas                          |
| Label                | Nominal simple                                |
| One-Hot              | Nominal sin orden                             |
| Binary               | Muchas categor√≠as                             |
| Hashing              | Gran volumen, menos interpretabilidad         |
| Target               | Regresi√≥n, correlaci√≥n con variable objetivo  |

---

## üß¨ 2 : Reducci√≥n de Caracter√≠sticas

La selecci√≥n de caracter√≠sticas permite eliminar variables irrelevantes o redundantes, mejorando la eficiencia y precisi√≥n del modelo.

---

### üß† Categor√≠as Principales

| M√©todo       | Basado en...             | ¬øUsa modelo? | Caracter√≠sticas clave                          |
|--------------|--------------------------|--------------|------------------------------------------------|
| Filter       | Estad√≠sticas individuales| ‚ùå No         | R√°pido, independiente del modelo               |
| Wrapper      | Rendimiento del modelo   | ‚úÖ S√≠         | Eval√∫a subconjuntos, computacionalmente costoso|
| Embedded     | Aprendizaje interno del modelo | ‚úÖ S√≠   | Usa regularizaci√≥n o importancia autom√°tica    |

---

### üîé A. Filter Methods

Eval√∫an cada variable por separado, usando estad√≠sticas para seleccionar las m√°s relevantes.

#### üìå T√©cnicas comunes:
- **Chi-cuadrado (œá¬≤)** ‚Äì para variables categ√≥ricas
- **ANOVA F-test** ‚Äì para clasificaci√≥n
- **Correlaci√≥n de Pearson** ‚Äì para regresi√≥n y datos num√©ricos

#### üì¶ Ejemplo en Python:

```python
from sklearn.feature_selection import SelectKBest, chi2, f_classif

# Chi-cuadrado para clasificaci√≥n
selector = SelectKBest(score_func=chi2, k=5)
X_new = selector.fit_transform(X, y)

# ANOVA F-test
selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X, y)
```

---

### üîÅ B. Wrapper Methods

Eval√∫an **subconjuntos de caracter√≠sticas** entrenando modelos m√∫ltiples para encontrar combinaciones √≥ptimas.

#### üìå M√©todos t√≠picos:

| M√©todo    | Descripci√≥n                                                    |
|-----------|----------------------------------------------------------------|
| **SFS**   | Agrega features uno a uno que m√°s mejoran el modelo            |
| **BFS**   | Elimina features uno a uno hasta que el modelo empeore         |
| **SFFS**  | Forward con posibilidad de eliminar en pasos posteriores       |
| **SBFS**  | Backward con posibilidad de recuperar features eliminadas      |
| **RFE**   | Elimina recursivamente las menos importantes con un modelo     |

#### üì¶ Ejemplo con RFE:

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
selector = RFE(model, n_features_to_select=5)
X_selected = selector.fit_transform(X, y)
```

#### üì¶ Ejemplo con SFS (`mlxtend`):

```python
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LogisticRegression

sfs = SFS(LogisticRegression(),
          k_features=5,
          forward=True,  # False para backward
          floating=False,  # True para SFFS/SBFS
          scoring='accuracy',
          cv=5)

sfs = sfs.fit(X, y)
print("Mejores features:", sfs.k_feature_names_)
```

---

### üß≤ C. Embedded Methods (con Regularizaci√≥n)

Incorporan la selecci√≥n de caracter√≠sticas **dentro del proceso de entrenamiento** del modelo. Usan penalizaciones para reducir el impacto de variables menos √∫tiles.

#### üß† ¬øQu√© es Regularizaci√≥n?

La regularizaci√≥n agrega una penalizaci√≥n al modelo para evitar sobreajuste y reducir la complejidad. Esto puede "forzar" a ciertos coeficientes a valores muy bajos o incluso cero.

#### üî∂ Lasso Regression (L1)

- Penaliza con la **suma de los valores absolutos** de los coeficientes.
- Puede forzar coeficientes a cero ‚Üí selecci√≥n autom√°tica de caracter√≠sticas.

$$
\text{Loss}_{L1} = RSS + \alpha \sum |w_i|
$$

```python
from sklearn.linear_model import LassoCV

model = LassoCV()
model.fit(X, y)
print(model.coef_)  # Coeficientes importantes se mantienen
```

---

#### üî∑ Ridge Regression (L2)

- Penaliza con la **suma de los cuadrados** de los coeficientes.
- Reduce la magnitud de los coeficientes, pero no los lleva a cero.
- √ötil cuando hay **colinealidad** entre variables.

$$
\text{Loss}_{L2} = RSS + \alpha \sum w_i^2
$$

```python
from sklearn.linear_model import RidgeCV

ridge = RidgeCV(alphas=[0.1, 1.0, 10.0])
ridge.fit(X, y)
print(ridge.coef_)
```

---

#### üîÄ ElasticNet

- Combinaci√≥n de **L1 (Lasso)** y **L2 (Ridge)**.
- Controla el equilibrio entre selecci√≥n de variables y regularizaci√≥n suave.

$$
\text{Loss}_{ElasticNet} = RSS + \alpha (r \sum |w_i| + (1 - r) \sum w_i^2)
$$

```python
from sklearn.linear_model import ElasticNetCV

enet = ElasticNetCV()
enet.fit(X, y)
print(enet.coef_)
```

---

#### üå≤ Importancia basada en √°rboles

Modelos como Random Forest o XGBoost calculan la importancia de cada variable de forma autom√°tica.

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X, y)

importances = model.feature_importances_
```

---

## üåà 3: Dimensionality Reduction: PCA y t-SNE

### üß† ¬øPor qu√© reducir dimensiones?

- Visualizaci√≥n m√°s clara de los datos.  
- Eliminar ruido o colinealidad.  
- Acelerar entrenamiento de modelos.  
- Mejorar generalizaci√≥n y evitar overfitting.  

---

### üîç ¬øPor que son no supervisados?

Las t√©cnicas de reducci√≥n de dimensionalidad **no supervisadas** (como PCA y t-SNE) **no** utilizan las etiquetas $(y$) de los datos al buscar sus nuevas representaciones. En lugar de aprender una funci√≥n $(f: X \to y$), estas t√©cnicas:

1. Analizan √∫nicamente la **estructura interna** de las caracter√≠sticas $(X$).  
2. Encuentran combinaciones o proyecciones (componentes principales en PCA, distribuci√≥n probabil√≠stica local en t-SNE) que **maximizan la retenci√≥n de informaci√≥n** o **conservan distancias** entre puntos.  
3. Operan sin gu√≠a de salidas deseadas, extrayendo patrones de forma **exploratoria** y **descriptiva**.

---

### üìâ PCA (Principal Component Analysis)

**PCA** busca proyectar los datos en un nuevo espacio de menor dimensi√≥n **maximizando la varianza**.
#### ‚öôÔ∏è Fundamento Matem√°tico

1. Centrar los datos (media = 0).
2. Calcular la **matriz de covarianza**.
3. Calcular los **eigenvectors** y **eigenvalores**.
4. Elegir los componentes con mayor varianza.
5. Proyectar los datos en el nuevo subespacio.

$$
X_{pca} = X \cdot W_k
$$

- $X$: matriz original
- $W_k$: los $k$ autovectores principales

---

#### üìå ¬øCu√°ndo usar PCA?

- Datos **num√©ricos**.
- Relaci√≥n **lineal** entre variables.
- Objetivo: **reducci√≥n**, **visualizaci√≥n**, o **descorrelaci√≥n**.

---

#### üíª C√≥digo PCA en Python

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Datos simulados
X = df.drop('target', axis=1)
y = df['target']

# Escalado previo
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Visualizaci√≥n
plt.figure(figsize=(6, 5))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette='Set2')
plt.title("PCA - 2 Componentes")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()
```

---

#### üßÆ ¬øCu√°ntos componentes elegir?

```python
pca = PCA().fit(X_scaled)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel("N√∫mero de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.title("Varianza explicada por PCA")
plt.grid()
plt.show()
```

---

### üé® t-SNE (t-distributed Stochastic Neighbor Embedding)

**t-SNE** es una t√©cnica **no lineal** que preserva relaciones locales entre puntos para **visualizaci√≥n** en 2D o 3D.

- Basado en **distribuciones de probabilidad**.
- Ideal para clusters no lineales.

---

#### üíª C√≥digo t-SNE en Python

```python
from sklearn.manifold import TSNE

# t-SNE requiere datos escalados
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Visualizaci√≥n
plt.figure(figsize=(6, 5))
sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette='coolwarm')
plt.title("t-SNE - 2 Componentes")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.show()
```

---

### ‚úÖ Conclusiones

#### üß† Diferencias vs PCA

| Caracter√≠stica       | PCA               | t-SNE                  |
|----------------------|-------------------|------------------------|
| Tipo                 | Lineal            | No lineal              |
| Uso principal        | Reducci√≥n general | Visualizaci√≥n          |
| Velocidad            | R√°pido            | Lento (grandes datos)  |
| Interpretabilidad    | Alta              | Baja                   |
| Conserva distancias  | Global            | Local                  |

- Usa **PCA** si necesitas interpretaci√≥n, velocidad, o reducci√≥n antes de modelos.
- Usa **t-SNE** si deseas una **visualizaci√≥n intuitiva** de la estructura interna de los datos.

> üåü A veces es √∫til combinar ambos: aplicar PCA para reducir a 50 dimensiones y luego t-SNE a 2D.


---
# Parte 3 :üìä Tabla Global de Modelos de Machine Learning en Scikit-learn

Gu√≠a de referencia r√°pida organizada por tipo de modelo, con columnas clave para selecci√≥n, preprocesamiento, interpretaci√≥n y ajuste.

---

## ‚úÖ Clasificaci√≥n

| Modelo                  | Clase `scikit-learn`                          | Lineal | Escalado Necesario | Sensible a Outliers | Interpretable | Predice Probabilidades | Soporta Multiclase | Hiperpar√°metros Clave          |
|-------------------------|-----------------------------------------------|--------|---------------------|---------------------|----------------|------------------------|---------------------|-------------------------------|
| Logistic Regression     | `sklearn.linear_model.LogisticRegression`     | S√≠     | ‚úÖ S√≠               | Alta                | ‚úÖ Alta        | ‚úÖ S√≠                  | ‚úÖ S√≠               | `C`, `penalty`, `solver`      |
| K-Nearest Neighbors     | `sklearn.neighbors.KNeighborsClassifier`      | No     | ‚úÖ S√≠               | Alta                | Media          | ‚ùå No                 | ‚úÖ S√≠               | `n_neighbors`, `weights`      |
| Decision Tree           | `sklearn.tree.DecisionTreeClassifier`         | No     | ‚ùå No               | ‚úÖ Baja             | Media          | ‚ùå No                 | ‚úÖ S√≠               | `max_depth`, `min_samples_split` |
| Random Forest           | `sklearn.ensemble.RandomForestClassifier`     | No     | ‚ùå No               | ‚úÖ Baja             | ‚ùå Baja         | ‚úÖ S√≠                  | ‚úÖ S√≠               | `n_estimators`, `max_depth`   |
| Support Vector Machine  | `sklearn.svm.SVC`                              | S√≠     | ‚úÖ S√≠               | Alta                | ‚ùå Baja         | ‚úÖ S√≠ (`probability`)  | ‚úÖ S√≠               | `C`, `kernel`, `gamma`        |
| Gaussian Naive Bayes    | `sklearn.naive_bayes.GaussianNB`              | S√≠     | Recomendado        | Alta                | ‚úÖ Alta        | ‚úÖ S√≠                  | ‚úÖ S√≠               | -                             |

---

## üìà Regresi√≥n

| Modelo                  | Clase `scikit-learn`                          | Lineal | Escalado Necesario | Sensible a Outliers | Interpretable | Soporta Multioutput | Hiperpar√°metros Clave           |
|-------------------------|-----------------------------------------------|--------|---------------------|---------------------|----------------|----------------------|-------------------------------|
| Linear Regression       | `sklearn.linear_model.LinearRegression`       | S√≠     | ‚úÖ S√≠               | Alta                | ‚úÖ Alta        | ‚úÖ S√≠                | -                             |
| Ridge                   | `sklearn.linear_model.Ridge`                  | S√≠     | ‚úÖ S√≠               | Alta                | ‚úÖ Alta        | ‚úÖ S√≠                | `alpha`                       |
| Lasso                   | `sklearn.linear_model.Lasso`                  | S√≠     | ‚úÖ S√≠               | Alta                | ‚úÖ Alta        | ‚úÖ S√≠                | `alpha`                       |
| K-Nearest Regressor     | `sklearn.neighbors.KNeighborsRegressor`       | No     | ‚úÖ S√≠               | Alta                | Media          | ‚úÖ S√≠                | `n_neighbors`, `weights`      |
| Decision Tree Regressor| `sklearn.tree.DecisionTreeRegressor`          | No     | ‚ùå No               | ‚úÖ Baja             | Media          | ‚úÖ S√≠                | `max_depth`, `min_samples_split` |
| Random Forest Regressor| `sklearn.ensemble.RandomForestRegressor`      | No     | ‚ùå No               | ‚úÖ Baja             | ‚ùå Baja         | ‚úÖ S√≠                | `n_estimators`, `max_depth`   |
| Support Vector Regressor| `sklearn.svm.SVR`                             | No     | ‚úÖ S√≠               | Alta                | ‚ùå Baja         | ‚ùå No                | `C`, `kernel`, `epsilon`      |

---

## üìä Clustering (No Supervisado)

| Modelo                  | Clase `scikit-learn`                          | Escalado Necesario | Sensible a Outliers | Necesita k | Predice Cluster (`predict`) | M√©tricas comunes             |
|-------------------------|-----------------------------------------------|---------------------|---------------------|------------|------------------------------|------------------------------|
| KMeans                  | `sklearn.cluster.KMeans`                      | ‚úÖ S√≠               | ‚úÖ Alta             | ‚úÖ S√≠     | ‚úÖ S√≠                        | Inertia, Silhouette Score    |
| DBSCAN                  | `sklearn.cluster.DBSCAN`                      | ‚úÖ S√≠               | ‚úÖ Alta             | ‚ùå No     | ‚ùå No (`labels_`)            | Silhouette, Davies-Bouldin   |
| Agglomerative Clustering| `sklearn.cluster.AgglomerativeClustering`     | ‚úÖ S√≠               | ‚úÖ Alta             | ‚úÖ S√≠     | ‚ùå No (`labels_`)            | Silhouette, Dendrogram       |
| Gaussian Mixture (EM)   | `sklearn.mixture.GaussianMixture`            | ‚úÖ S√≠               | Alta                | ‚úÖ S√≠     | ‚úÖ S√≠                        | Log-likelihood, AIC, BIC     |

---

## üß∞ Herramientas Complementarias

- `Pipeline`: Secuencia de pasos (`StandardScaler` ‚Üí `Model`).
- `GridSearchCV`: B√∫squeda exhaustiva de hiperpar√°metros.
- `cross_val_score`: Validaci√≥n cruzada r√°pida.
- `train_test_split`: Separar `X_train`, `X_test`, `y_train`, `y_test`.
- `StandardScaler`, `MinMaxScaler`: Normalizaci√≥n / escalado.
- `OneHotEncoder`: Para variables categ√≥ricas.
- `SelectKBest`, `PCA`: Selecci√≥n / reducci√≥n de caracter√≠sticas.

> üß† **Consejo:** Usa `Pipeline` junto con `GridSearchCV` para combinar normalizaci√≥n, selecci√≥n de variables y modelo en un solo flujo reproducible.


---
# ü§ñ Parte 5: Modelado Supervisado
## üìà Regresi√≥n Lineal

Modelo b√°sico con una variable:

$$
\hat{y} = mx + b
$$

### ‚úÖ Supuestos

1. Linealidad
2. Homocedasticidad
3. Independencia de errores
4. Normalidad de los residuos

### üìå Simple Linear Regression

```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([1.5, 2.0, 3.5, 3.7, 4.5])

model = LinearRegression()
model.fit(x, y)

y_pred = model.predict(x)
plt.scatter(x, y, label="Datos reales")
plt.plot(x, y_pred, color="red", label="Regresi√≥n")
plt.legend()
plt.title("Regresi√≥n Lineal Simple")
plt.show()
```

---

### üìå Multiple Linear Regression

```python
data = pd.DataFrame({
    "horas_estudio": [1, 2, 3, 4, 5],
    "horas_sue√±o": [8, 7, 6, 5, 4],
    "nota": [60, 65, 70, 75, 80]
})
X = data[["horas_estudio", "horas_sue√±o"]]
y = data["nota"]

model = LinearRegression()
model.fit(X, y)
print("Coeficientes:", model.coef_)
print("Intercepto:", model.intercept_)
```

---

### üìä Correlaci√≥n

```python
import seaborn as sns
sns.heatmap(data.corr(), annot=True, cmap="coolwarm")
plt.title("Matriz de Correlaci√≥n")
plt.show()
```

---

### üìê Evaluaci√≥n del Modelo

* MAE: $\frac{1}{n} \sum |y - \hat{y}|$
- MSE: $\frac{1}{n} \sum (y - \hat{y})^2$
- RMSE: $\sqrt{MSE}$
- R¬≤ Score: Varianza explicada por el modelo

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred = model.predict(X)
print("MAE:", mean_absolute_error(y, y_pred))
print("MSE:", mean_squared_error(y, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y, y_pred)))
print("R¬≤:", r2_score(y, y_pred))
```

---

## üßÆ OLS vs Gradient Descent

| M√©todo              | OLS                          | Gradient Descent (GD)        |
|---------------------|-------------------------------|-------------------------------|
| Tipo                | Anal√≠tico                     | Iterativo                     |
| Exactitud           | Soluci√≥n exacta               | Aproximaci√≥n                  |
| Velocidad (pocos datos) | R√°pido                     | M√°s lento                     |
| Velocidad (big data)    | Lento (√°lgebra matricial)  | Escalable                     |
| Requiere ajuste     | No                            | S√≠ (learning rate, epochs)    |

### OLS F√≥rmula:

$$
\beta = (X^T X)^{-1} X^T y
$$

### GD F√≥rmulas:

$$
m \leftarrow m - \eta \cdot \frac{\partial}{\partial m} \text{Loss},\quad
b \leftarrow b - \eta \cdot \frac{\partial}{\partial b} \text{Loss}
$$

---

## üîÅ Regresi√≥n Log√≠stica

### üìò Definici√≥n

Predice una variable categ√≥rica (binaria) usando una transformaci√≥n sigmoide.
$$
P(y = 1 | x) = \frac{1}{1 + e^{-(b + m_1 x_1 + \dots + m_n x_n)}}
$$

### üß™ Implementaci√≥n

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(class_weight='balanced')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

### üìä M√©tricas y Matriz de Confusi√≥n

- Accuracy
- Precision, Recall
- F1 Score
- ROC-AUC

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

> ‚ö†Ô∏è Usa `predict_proba` para obtener probabilidades y ajustar el threshold si es necesario.
---

### üìà ROC Curve & AUC

```python
from sklearn.metrics import roc_curve, roc_auc_score

y_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], '--', color='gray')
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC Curve")
plt.legend()
plt.show()
```

---

### ‚öôÔ∏è Ajuste de Threshold

```python
y_pred_custom = (y_proba > 0.3).astype(int)
```

---

## üß† Consideraciones Avanzadas

- Usa `class_weight="balanced"` si hay desbalance.
- Revisa correlaciones para evitar multicolinealidad.
- Normaliza si las variables tienen escalas diferentes.
- Visualiza con curvas ROC y matriz de confusi√≥n.

---

## üîç K-Nearest Neighbors (KNN)

### üß† Idea Principal

KNN predice un resultado **basado en los K vecinos m√°s cercanos** en el espacio de caracter√≠sticas.

- **KNN Clasificaci√≥n**: vota la clase m√°s com√∫n entre los vecinos ( 0 o 1).
- **KNN Regresi√≥n**: promedia el valor de salida de los vecinos( valores num√©ricos como estrellas  de una pel√≠cula) .

### üìè Recomendaciones generales

- Siempre **escalar los datos** (StandardScaler o MinMaxScaler) antes de usar KNN.
- Seleccionar el valor √≥ptimo de K es **cr√≠tico** (ni muy bajo ni muy alto).
- El par√°metro `weights` puede cambiar significativamente el resultado:
  - `'uniform'`: todos los vecinos tienen el mismo peso.
  - `'distance'`: los vecinos m√°s cercanos tienen m√°s influencia.

---

### üìò KNN para Clasificaci√≥n

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# Ajustar K autom√°ticamente
param_grid = {'n_neighbors': range(1, 21), 'weights': ['uniform', 'distance']}
grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)

print("Mejor K:", grid.best_params_['n_neighbors'])
print("Pesos usados:", grid.best_params_['weights'])

# Evaluar
y_pred = grid.predict(X_test)
print(classification_report(y_test, y_pred))
```

---

### üìò KNN para Regresi√≥n

```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

reg = KNeighborsRegressor(n_neighbors=5, weights='distance')
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
```

---

### üß™ Comparaci√≥n de pesos en regresi√≥n

```python
for weight in ['uniform', 'distance']:
    model = KNeighborsRegressor(n_neighbors=5, weights=weight)
    model.fit(X_train, y_train)
    print(f"{weight} MSE:", mean_squared_error(y_test, model.predict(X_test)))
```

---

### üß† Elecci√≥n de K

- K muy peque√±o ‚Üí sobreajuste (modelo muy flexible).
- K muy grande ‚Üí subajuste (modelo demasiado general).
- Se recomienda probar con `GridSearchCV` o validaci√≥n cruzada.

---

## üå≥ √Årboles de Decisi√≥n (Decision Trees)

### üß† Idea Principal

Dividen el espacio de decisiones en **reglas tipo s√≠/no** seg√∫n las caracter√≠sticas m√°s importantes.

- Se crean nodos que maximizan la **ganancia de informaci√≥n** o reducen la **impureza de Gini**.

---

### üìò Criterios de divisi√≥n

- **Gini Impurity**:
  $$
  Gini = 1 - \sum_{i=1}^C p_i^2
  $$

- **Entropy / Information Gain**:
  $$
  Entropy = - \sum_{i=1}^C p_i \log_2 p_i
  $$

  $$
  IG = Entropy_{parent} - \sum \left( \frac{n_{child}}{n_{total}} \cdot Entropy_{child} \right)
  $$

---

### üíª Implementaci√≥n

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

model = DecisionTreeClassifier(criterion='gini', max_depth=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
plot_tree(model, filled=True, feature_names=X.columns)
plt.show()
```

---

### ‚öôÔ∏è Hiperpar√°metros clave

- `criterion`: `'gini'` o `'entropy'`
- `max_depth`: profundidad m√°xima del √°rbol
- `min_samples_split`: m√≠nimo de muestras para dividir un nodo
- `min_samples_leaf`: m√≠nimo de muestras en una hoja

---

### ‚úÖ Ventajas y Desventajas

| Pros                           | Contras                             |
|--------------------------------|--------------------------------------|
| F√°cil de interpretar           | Tienden al sobreajuste               |
| No necesita normalizaci√≥n      | Sensibles a peque√±os cambios         |
| Acepta variables categ√≥ricas   | √Årboles muy grandes son dif√≠ciles de manejar |

---

> üå± Para mejorar los √°rboles individuales, se usan **Random Forests** y **Boosting**, que se explicar√°n m√°s adelante.

---

## üéØEnsemble methods


En Machine Learning, **los m√©todos de ensamble** combinan m√∫ltiples modelos (a menudo llamados *estimadores base*) para crear un modelo m√°s robusto y preciso que cualquiera de sus partes por separado.

> üìå *"Un conjunto de modelos d√©biles puede formar un modelo fuerte."*

---

### üéØ ¬øPor qu√© usar Ensembles?

- üîÅ Reducen **varianza** (overfitting).
- üéØ Disminuyen **sesgo** (underfitting).
- üìà Mejoran la **precisi√≥n y estabilidad** del modelo.
- üõ°Ô∏è Son m√°s **resistentes al ruido** y a errores de muestreo.

---

### üîó Tipos Principales

| Tipo         | ¬øC√≥mo funciona?                                             | Ejemplo t√≠pico                |
|--------------|-------------------------------------------------------------|-------------------------------|
| **Bagging**  | Entrena varios modelos **en paralelo** con muestras distintas (bootstrapped). | Random Forest                 |
| **Boosting** | Entrena modelos **en secuencia**, cada uno corrige los errores del anterior. | AdaBoost, Gradient Boosting  |
| **Stacking** | Combina modelos diferentes y usa otro modelo (meta-modelo) para hacer la predicci√≥n final. | Modelos de mezcla (blending) |

---

### üîç Comparaci√≥n General

| M√©todo     | Reduce varianza | Reduce sesgo | Paralelizable | Ejemplo                  |
|------------|------------------|---------------|----------------|---------------------------|
| Bagging    | ‚úÖ                | ‚ùå             | ‚úÖ              | Random Forest             |
| Boosting   | ‚ùå                | ‚úÖ             | ‚ùå              | XGBoost, AdaBoost         |
| Stacking   | ‚úÖ‚úÖ              | ‚úÖ‚úÖ           | ‚ùå              | Meta-ensemble personalizado |

---

### A) Bagging Methods

#### üå≤Random Forest
El **Random Forest** es un algoritmo de aprendizaje supervisado basado en √°rboles de decisi√≥n. Es parte de la familia de m√©todos de ensamble, espec√≠ficamente del tipo **bagging**.

---

##### üéØ ¬øQu√© es?

Un **Random Forest** entrena m√∫ltiples √°rboles de decisi√≥n sobre diferentes subconjuntos del conjunto de datos (mediante muestreo con reemplazo) y luego combina sus predicciones:

- En clasificaci√≥n: toma la **votaci√≥n mayoritaria**
- En regresi√≥n: toma el **promedio** de las predicciones

---

##### üîç ¬øPor qu√© usar Random Forest?

- Reduce el **overfitting** de los √°rboles individuales
- Proporciona una buena estimaci√≥n de la importancia de las caracter√≠sticas
- Robusto ante datos ruidosos y valores at√≠picos

---

##### üîß Principales Hiperpar√°metros

| Par√°metro              | Descripci√≥n                                               |
|------------------------|-----------------------------------------------------------|
| `n_estimators`         | N√∫mero de √°rboles en el bosque                            |
| `max_depth`            | Profundidad m√°xima de cada √°rbol                          |
| `max_features`         | N√∫mero de features consideradas en cada split             |
| `min_samples_split`    | M√≠nimo de muestras requeridas para dividir un nodo        |
| `min_samples_leaf`     | M√≠nimo de muestras en una hoja                            |
| `bootstrap`            | Si se usan muestras con reemplazo (`True` por defecto)    |

---

##### üíª Ejemplo en Python

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Datos de ejemplo
X, y = load_iris(return_X_y=True)

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Modelo
rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
rf.fit(X_train, y_train)

# Predicci√≥n y evaluaci√≥n
y_pred = rf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

##### üåø Importancia de Caracter√≠sticas

Random Forest permite evaluar la **importancia relativa** de cada variable en las decisiones del modelo.

```python
import pandas as pd
import matplotlib.pyplot as plt

feature_importance = rf.feature_importances_
features = load_iris().feature_names
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})
importance_df.sort_values(by="Importance", ascending=True).plot.barh(x='Feature', y='Importance')
plt.title("Importancia de Caracter√≠sticas")
plt.show()
```

---

#### ‚úÖ Ventajas

- Funciona bien con datos no lineales y con muchas variables
- Poca necesidad de preprocesamiento
- Puede manejar datos faltantes (algunas implementaciones)

#### ‚ö†Ô∏è Desventajas

- Dif√≠cil de interpretar individualmente (modelo tipo "caja negra")
- Lento para predicci√≥n en tiempo real con muchos √°rboles
- Puede consumir mucha memoria

---

#### üß† Cu√°ndo usarlo

- Problemas de clasificaci√≥n o regresi√≥n donde se necesita robustez
- Cuando otros modelos individuales tienen alto **overfitting**
- Como baseline potente antes de probar modelos m√°s complejos

---

#### üìå Notas adicionales

- Si hay **desequilibrio de clases**, puedes usar `class_weight='balanced'`
- Para regresi√≥n, utiliza `RandomForestRegressor` con la misma l√≥gica

---

### B) üöÄ Boosting Methods 

Boosting es una t√©cnica de ensamble que **combina varios modelos d√©biles** (como √°rboles peque√±os) para formar un modelo fuerte. A diferencia de bagging (como Random Forest), el entrenamiento es **secuencial**: cada modelo corrige los errores del anterior.

---

#### üß† Idea Principal

1. Entrena un modelo d√©bil (e.g., √°rbol).
2. Eval√∫a los errores.
3. Entrena otro modelo **enfocado en los errores**.
4. Repite, combinando todos los modelos con **pesos**.

> Se usa mucho en tareas donde se necesita **alta precisi√≥n** (competencias, bancos, medicina).

---

#### üì¶ Tipos de Boosting

##### 1. üéØ AdaBoost (Adaptive Boosting)

- Asigna m√°s peso a los errores.
- Usa modelos d√©biles (usualmente √°rboles de decisi√≥n con profundidad 1).
- Actualiza los pesos en cada iteraci√≥n.

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0
)
ada.fit(X_train, y_train)
```

---

##### 2. üå≥ Gradient Boosting

- Optimiza una funci√≥n de p√©rdida usando gradientes.
- M√°s preciso que AdaBoost, pero m√°s lento.
- Puede sobreajustar si no se regula bien.

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
gb.fit(X_train, y_train)
```

---

##### 3. ‚ö° XGBoost (Extreme Gradient Boosting)

- Optimizaci√≥n de Gradient Boosting.
- Muy usado en Kaggle y producci√≥n.
- R√°pido, regularizado, eficiente.

```python
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    use_label_encoder=False,
    eval_metric='logloss'
)
xgb.fit(X_train, y_train)
```

---

##### 4. üß† LightGBM

- Usa histogramas y hojas en lugar de niveles.
- M√°s r√°pido en datasets grandes.
- Requiere datos limpios y sin categor√≠as codificadas mal.

```python
from lightgbm import LGBMClassifier

lgbm = LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
lgbm.fit(X_train, y_train)
```

---

#### ‚úÖ Ventajas

- Alta precisi√≥n.
- Reduce sesgo.
- √ötil en datos tabulares.

#### ‚ö†Ô∏è Desventajas

- Lento si no se optimiza.
- M√°s propenso a sobreajuste.
- Dif√≠cil de interpretar (especialmente XGBoost).

---

#### üß™ Consejos Pr√°cticos

- Ajusta el n√∫mero de estimadores y `learning_rate` juntos.
- Usa `early_stopping_rounds` en XGBoost o LightGBM para evitar overfitting.
- Visualiza la importancia de variables (`feature_importances_`).

---

#### üìä Visualizaci√≥n de Importancia

```python
import matplotlib.pyplot as plt

importances = xgb.feature_importances_
plt.bar(range(len(importances)), importances)
plt.title("Importancia de Caracter√≠sticas (XGBoost)")
plt.show()
```

> ‚úÖ Boosting es ideal cuando se busca precisi√≥n m√°xima.  
> üîç ¬°Pero ojo con el tiempo de c√≥mputo y el sobreajuste!

---

### D) üß± Stacking Methods


**Stacking (Stacked Generalization)** es una t√©cnica de ensamblado donde m√∫ltiples modelos (denominados *base learners*) son entrenados y sus predicciones son usadas como entrada de un **modelo meta** (*meta learner*), que aprende a combinarlas de forma √≥ptima.

---

#### üß† ¬øPor qu√© usar Stacking?

- Aprovecha la **diversidad de modelos** para mejorar el rendimiento.
- El *meta-modelo* aprende de los errores y aciertos de cada *base learner*.
- Puede mejorar el rendimiento en comparaci√≥n con m√©todos individuales o bagging/boosting.

---

#### ‚öôÔ∏è Arquitectura de Stacking

```
X_train ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Model 1 ‚îÄ‚îê
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Model 2 ‚îÄ‚î§
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Model 3 ‚îÄ‚î§‚îÄ‚îÄ> Meta-model (e.g. Logistic Regression)
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ...     ‚îÄ‚îò
```

- Los modelos base se entrenan con el conjunto de entrenamiento.
- El modelo meta se entrena con las predicciones de los modelos base.
- Puede usarse validaci√≥n cruzada para evitar sobreajuste.

---

#### üî¢ ¬øQu√© modelos usar?

- **Base learners**: modelos diversos (√°rboles, regresiones, KNN, etc.).
- **Meta learner**: modelo simple como regresi√≥n log√≠stica o ridge regression.

---

#### üíª Ejemplo en Python con Scikit-learn

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Dataset
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Modelos base
estimators = [
    ('dt', DecisionTreeClassifier(max_depth=3)),
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('svm', SVC(probability=True))
]

# Modelo de ensamblado
stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()
)

# Entrenar
stack.fit(X_train, y_train)
print("Accuracy:", stack.score(X_test, y_test))
```

---

#### üß† Recomendaciones

- Usa modelos base que generen predicciones distintas.
- Asegura que el *meta-modelo* no tenga acceso a los datos originales, solo a las predicciones.
- Usa validaci√≥n cruzada interna para generar las predicciones de entrenamiento del *meta-modelo*.

---

#### ‚úÖ Ventajas

| Beneficio                 | Descripci√≥n |
|---------------------------|-------------|
| Alta performance          | Puede superar a modelos individuales |
| Flexibilidad              | Puedes mezclar cualquier tipo de modelo |
| Aprovecha especializaci√≥n| Cada modelo puede enfocarse en un tipo de patr√≥n |

#### ‚ö†Ô∏è Limitaciones

| Desventaja               | Descripci√≥n |
|--------------------------|-------------|
| Complejidad computacional| Entrena m√∫ltiples modelos |
| Riesgo de overfitting    | Si no se aplica correctamente |
| Dificultad de interpretaci√≥n | Dif√≠cil explicar decisiones |

---

#### üß™ Variaciones

- **Blending**: variante m√°s simple de stacking que usa un conjunto holdout en vez de validaci√≥n cruzada.
- **Multilayer Stacking**: anidaci√≥n de varios niveles de modelos base y meta.

---

## üß† Naive Bayes Classifier

`Naive Bayes` es una familia de algoritmos de clasificaci√≥n basada en el **Teorema de Bayes**. Es simple pero muy eficaz para tareas como clasificaci√≥n de texto (spam, an√°lisis de sentimientos, etc.).

---

## üìê Teorema de Bayes

El **Teorema de Bayes** permite calcular la probabilidad de un evento A dado un evento B:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

En palabras sencillas:

> ¬øCu√°l es la probabilidad de que algo sea **A** si observamos **B**?

---

## üß† Naive Bayes: ¬øpor qu√© "naive"?

Se le llama *naive* (ingenuo) porque **asume que las variables predictoras son independientes entre s√≠**, lo cual rara vez es cierto, pero en la pr√°ctica funciona muy bien.

### Aplicaci√≥n cl√°sica: clasificaci√≥n de texto

Usamos `Naive Bayes` para predecir si un mensaje es **spam o no spam**, bas√°ndonos en las palabras que contiene.

---

## üî§ CountVectorizer: convertir texto a n√∫meros

Antes de entrenar el modelo, debemos convertir el texto en una **matriz num√©rica**. `CountVectorizer` convierte las palabras en conteos:

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "me encanta el machine learning",
    "el aprendizaje autom√°tico es fascinante",
    "odio el spam",
    "el spam es molesto"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

Cada fila representa un documento (frase) y cada columna una palabra.

---

## ü§ñ Entrenar un modelo con MultinomialNB

El modelo `MultinomialNB` es una variante de Naive Bayes dise√±ada para datos **discretos** como el conteo de palabras.

```python
from sklearn.naive_bayes import MultinomialNB

# Etiquetas: 0 = ham, 1 = spam
y = [0, 0, 1, 1]

model = MultinomialNB()
model.fit(X, y)
```

---

## üîç Hacer predicciones

```python
test = ["me molesta el spam"]
X_test = vectorizer.transform(test)

pred = model.predict(X_test)
print(pred)  # Resultado: [1] => spam
```

---

## üìà ¬øPor qu√© usar Naive Bayes?

‚úÖ Muy r√°pido y eficiente  
‚úÖ Funciona bien con datos de texto  
‚úÖ Ideal para datos grandes y dispersos  
‚ö†Ô∏è No captura relaciones complejas entre variables

---

## üß™ M√©tricas para evaluaci√≥n

Usa las mismas m√©tricas que en clasificaci√≥n:

- Accuracy
- Precision
- Recall
- F1-score

```python
from sklearn.metrics import classification_report

print(classification_report(y, model.predict(X)))
```

---

## üìå Recomendaciones

- Funciona mejor cuando las caracter√≠sticas son independientes.
- Si los datos tienen muchas ceros (como en texto), `MultinomialNB` o `BernoulliNB` son buenas opciones.
- Para texto, combinar `CountVectorizer` o `TfidfVectorizer` con `Naive Bayes` suele dar buenos resultados.


---
# ü§ñ Parte 6: Modelado No-Supervisado

## üß† K-Means Clustering

K-Means es un algoritmo de **aprendizaje no supervisado** utilizado para agrupar datos similares. Busca dividir los datos en **K grupos (clusters)**, donde cada punto pertenece al cluster con el centro m√°s cercano (media).

---

### üöÄ ¬øC√≥mo Funciona?

1. Se eligen **K centroides** aleatoriamente.
2. Se asigna cada punto al centroide m√°s cercano.
3. Se recalculan los centroides como la media de los puntos asignados.
4. Se repite el proceso hasta que los centroides no cambien (o cambien muy poco).

---

### üìê F√≥rmula de Distancia Euclidiana

$$
\text{dist}(x, \mu) = \sqrt{\sum_{i=1}^{n} (x_i - \mu_i)^2}
$$

Donde:
- $( x )$ es el punto de datos
- $( \mu )$ es el centroide

---

### üì¶ Implementaci√≥n en Python

```python
from sklearn.cluster import KMeans
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Datos de ejemplo
df = pd.DataFrame({
    'x': [1, 1.5, 3, 5, 3.5, 4.5, 3.5],
    'y': [1, 2, 4, 7, 5, 5, 4.5]
})

# K-Means con 2 clusters
kmeans = KMeans(n_clusters=2, random_state=42)
df['cluster'] = kmeans.fit_predict(df[['x', 'y']])

# Visualizar resultados
sns.scatterplot(data=df, x='x', y='y', hue='cluster', palette='viridis')
plt.scatter(*kmeans.cluster_centers_.T, s=200, c='red', label='Centroides')
plt.legend()
plt.title("K-Means Clustering")
plt.show()
```

---

### ‚ùì C√≥mo Elegir K (N√∫mero de Clusters)

#### üìâ M√©todo del Codo (Elbow Method)

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

inertia = []
K = range(1, 10)

for k in K:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(df[['x', 'y']])
    inertia.append(model.inertia_)

plt.plot(K, inertia, 'o-')
plt.xlabel('N√∫mero de Clusters K')
plt.ylabel('Inercia (Suma de errores)')
plt.title('M√©todo del Codo')
plt.show()
```

---

### üß™ M√©tricas de Evaluaci√≥n

| M√©trica            | Descripci√≥n |
|--------------------|-------------|
| **Inercia**         | Suma de distancias cuadradas entre puntos y su centroide |
| **Silhouette Score** | Mide la separaci√≥n entre clusters (0 a 1, mejor si es cercano a 1) |

```python
score = silhouette_score(df[['x', 'y']], df['cluster'])
print("Silhouette Score:", score)
```

---

#### üß† Ventajas

- Simple y eficiente
- Funciona bien con clusters esf√©ricos

### ‚ö†Ô∏è Desventajas

- Sensible a outliers
- No funciona bien con clusters de forma no circular
- Hay que definir K previamente

---

#### üéØ Cu√°ndo usar K-Means

- Cuando tienes datos sin etiquetas y buscas estructura interna.
- Cuando quieres una soluci√≥n r√°pida de agrupamiento.

> üí° Tip: Escalar los datos con `StandardScaler` antes de aplicar K-Means puede mejorar los resultados.


---

# üéØ Parte 8: Recommender Systems

Los *Recommender Systems* (sistemas de recomendaci√≥n) son algoritmos que predicen qu√© elementos pueden gustarle a un usuario, bas√°ndose en su historial o en las preferencias de otros usuarios. Son ampliamente usados en plataformas como **Netflix**, **Amazon**, **Spotify**, etc.

---

## üß© Tipos principales de Recommender Systems

| Tipo               | ¬øC√≥mo funciona?                                                                 | Ejemplo                         |
|--------------------|----------------------------------------------------------------------------------|----------------------------------|
| Content-Based      | Analiza las caracter√≠sticas de los √≠tems que te han gustado y busca similares. | "Te gust√≥ esta pel√≠cula de acci√≥n, prueba esta otra del mismo g√©nero." |
| Collaborative Filtering | Busca patrones entre usuarios con gustos similares.                          | "Personas como t√∫ tambi√©n vieron..." |
| Hybrid             | Combina los dos m√©todos anteriores.                                             | "Recomendaci√≥n personalizada combinando gustos e historial de otros." |

---

## üîß Usando Surprise para Collaborative Filtering

La librer√≠a `surprise` es una herramienta sencilla para crear y evaluar modelos de filtrado colaborativo.

### Paso 1: Importar y cargar datos

```python
from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate

# Cargar dataset de ejemplo
data = Dataset.load_builtin('ml-100k')  # Ratings de pel√≠culas
```

### Paso 2: Crear y evaluar el modelo

```python
algo = SVD()  # Singular Value Decomposition (modelo basado en factores)

# Evaluar usando validaci√≥n cruzada
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
```

---

## üîç Hacer predicciones

Una vez entrenado, puedes predecir qu√© calificaci√≥n dar√≠a un usuario a un √≠tem espec√≠fico:

```python
trainset = data.build_full_trainset()
algo.fit(trainset)

# Predecir calificaci√≥n del usuario 196 para el √≠tem 302
pred = algo.predict(uid='196', iid='302')
print(pred.est)  # .est contiene la calificaci√≥n estimada
```

---

## üìå Recomendaciones finales

- Para sistemas m√°s robustos puedes combinar modelos (*hybrid systems*).
- Existen otros enfoques como *deep learning*, *item-based filtering* o *context-aware*.
- Siempre valida tus modelos con datos reales y mide su impacto.

___
# üß† Parte 8: Optimizaci√≥n de Hiperpar√°metros (Hyperparameter Tuning)

Elegir correctamente los hiperpar√°metros puede marcar la diferencia entre un modelo mediocre y uno excelente. Aqu√≠ se presentan los m√©todos m√°s utilizados para hacer tuning de manera eficaz.

---

## ‚öôÔ∏è ¬øQu√© es un hiperpar√°metro?

Par√°metros definidos antes del entrenamiento y no aprendidos directamente del modelo.

Ejemplos:
- `n_neighbors` en KNN
- `C`, `kernel` en SVM
- `alpha` en Lasso/Ridge
- `max_depth`, `min_samples_split` en √°rboles y Random Forest
- `learning_rate`, `n_estimators` en Boosting

---

## üîç M√©todos de Optimizaci√≥n

### üîÑ 1. Grid Search

Explora **todas las combinaciones posibles** dentro de un conjunto de valores definido.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid = GridSearchCV(SVC(), param_grid, cv=5)
grid.fit(X_train, y_train)
```

‚úÖ Exhaustivo  
‚ö†Ô∏è Costoso con muchos hiperpar√°metros

---

### üé≤ 2. Random Search

Explora **combinaciones aleatorias** dentro de un espacio definido. M√°s eficiente en grandes espacios.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sklearn.ensemble import RandomForestClassifier

param_dist = {'n_estimators': randint(50, 200)}
rand = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=10, cv=5)
rand.fit(X_train, y_train)
```

‚úÖ M√°s r√°pido que Grid  
‚ö†Ô∏è Puede omitir combinaciones √≥ptimas

---

### üìà 3. Bayesian Optimization (con `optuna`)

Modelo probabil√≠stico que **aprende del pasado** para proponer mejores combinaciones.

```python
import optuna
from sklearn.model_selection import cross_val_score

def objective(trial):
    C = trial.suggest_loguniform("C", 1e-3, 10)
    kernel = trial.suggest_categorical("kernel", ["linear", "rbf"])
    model = SVC(C=C, kernel=kernel)
    return cross_val_score(model, X, y, cv=5).mean()

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

print("Best params:", study.best_params)
```

‚úÖ Inteligente, eficiente  
‚ö†Ô∏è M√°s complejo de implementar

---

### üß¨ 4. Algoritmos Gen√©ticos (con `TPOT`)

Simulan evoluci√≥n biol√≥gica para encontrar hiperpar√°metros √≥ptimos (y a veces estructuras de modelo).

```python
from tpot import TPOTClassifier

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
```

‚úÖ Explora soluciones creativas  
‚ö†Ô∏è Lento, uso de CPU intensivo

---

### üîß Otros m√©todos posibles

| M√©todo              | Descripci√≥n breve                            |
|---------------------|-----------------------------------------------|
| Hyperband           | Variante de random search con early stopping |
| Successive Halving  | Eval√∫a muchas configuraciones y elimina pronto las peores |

---

## üìä Evaluaci√≥n de Resultados

Despu√©s de usar `GridSearchCV` o `RandomizedSearchCV`:

```python
model = grid  # o rand, etc.

print(model.best_estimator_)  # Mejor modelo completo
print(model.best_params_)     # Mejores hiperpar√°metros
print(model.best_score_)      # Mejor puntuaci√≥n de validaci√≥n cruzada
print(model.cv_results_)      # Resultados completos de todas las combinaciones
```

‚úÖ Usa `.cv_results_` para an√°lisis personalizados o gr√°ficas de rendimiento.

---

## ‚úÖ Resumen Comparativo

| M√©todo          | Velocidad | Eficiencia | Complejidad | Escenarios ideales                     |
|-----------------|-----------|------------|-------------|----------------------------------------|
| Grid Search     | Lenta     | Alta       | Baja        | Pocos par√°metros, espacio peque√±o      |
| Random Search   | R√°pida    | Media      | Baja        | Muchos par√°metros, tuning general      |
| Bayesian Opt.   | Media     | Muy alta   | Media/Alta  | Optimizaci√≥n precisa y eficiente       |
| Gen√©tico (TPOT) | Lenta     | Alta       | Alta        | AutoML, problemas grandes o complejos  |
