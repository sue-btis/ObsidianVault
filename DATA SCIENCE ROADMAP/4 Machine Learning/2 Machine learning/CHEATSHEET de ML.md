
# ğŸ§  Machine Learning & Data Science Cheatsheet

GuÃ­a exhaustiva y pedagÃ³gica para aprender desde cero a nivel intermedio-avanzado en ML y DS.

---

## ğŸ“¦ 1. ExploraciÃ³n, ManipulaciÃ³n y AnÃ¡lisis de Datos con Pandas

```python
import pandas as pd
```

### ğŸ§© Estructuras de Datos

```python
# Serie
serie = pd.Series([10, 20, 30], index=['a', 'b', 'c'])

# DataFrame
datos = {'Nombre': ['Ana', 'Luis', 'Juan'], 'Edad': [23, 45, 34]}
df = pd.DataFrame(datos)
```

### ğŸ“¥ Cargar y Crear DataFrames

```python
df = pd.read_csv('archivo.csv')
data = np.array([[1, 2, 3], [4, 5, 6]])
df = pd.DataFrame(data, columns=['A', 'B', 'C'])
```

### ğŸ” SelecciÃ³n de Datos

```python
df['A']         # Columna
df.iloc[0]      # Por posiciÃ³n
df.loc[0, 'A']  # Por etiqueta
```

### â• TransformaciÃ³n y Filtrado

```python
df['D'] = df['A'] + df['B']
df['E'] = df['B'].apply(lambda x: x * 2)
df = df[df['A'] > 2]
df.drop('D', axis=1, inplace=True)
```

### ğŸ“Š EstadÃ­sticas BÃ¡sicas

```python
df.mean(), df.median(), df.std()
df.describe()
df.corr(), df.cov()
```

### ğŸ”„ Agrupamiento y Agregaciones

```python
df.groupby('A').agg({'B': ['mean', 'sum'], 'C': 'max'})
df.agg({'A': ['mean'], 'B': ['min', 'max']})
```

### ğŸ“ˆ Pivot Tables

```python
df.pivot_table(values='Sales', index='City', columns='Year', aggfunc='sum')
```

### ğŸ”— Merge y Join

```python
pd.merge(df1, df2, on='ID', how='inner')
pd.concat([df1, df2], axis=0, ignore_index=True)
```

---

## ğŸ”¢ 2. ManipulaciÃ³n NumÃ©rica y Ãlgebra Lineal con NumPy

```python
import numpy as np
```

### ğŸ§® Crear Arrays

```python
v = np.array([1, 2, 3])
A = np.array([[1, 2], [3, 4]])
I = np.eye(3)
Z = np.zeros((2, 3))
```

### ğŸ”„ Transformaciones

```python
v.reshape((3, 1))
A.T  # Transpuesta
```

### ğŸ”— ConstrucciÃ³n y Broadcasting

```python
np.column_stack((col1, col2))
v + 5  # Broadcasting
np.sum(v)
```

### ğŸ’¥ Operaciones Elementales

```python
A + B
A * B
np.dot(A, B)
A @ B  # MultiplicaciÃ³n matricial
```

### ğŸ“ Ãlgebra Lineal

```python
np.linalg.inv(A)
np.linalg.det(A)
eig_vals, eig_vecs = np.linalg.eig(A)
np.linalg.solve(A, b)
```

---

## ğŸ”„ Flujo BÃ¡sico de un Proyecto de Machine Learning

1. **RecolecciÃ³n de datos**: desde CSV, APIs o bases de datos.
2. **Limpieza**: manejo de datos faltantes, duplicados, valores extremos.
3. **AnÃ¡lisis exploratorio**: `describe()`, `groupby()`, `value_counts()`.
4. **VisualizaciÃ³n**: `matplotlib`, `seaborn` para entender la distribuciÃ³n.
5. **PreparaciÃ³n de datos**: codificaciÃ³n, escalado, divisiÃ³n train/test.
6. **Entrenamiento del modelo**: scikit-learn (`LinearRegression`, `KNeighborsClassifier`, etc.).
7. **EvaluaciÃ³n del modelo**: mÃ©tricas (accuracy, MSE, F1-score...).
8. **Ajuste y validaciÃ³n cruzada**: `GridSearchCV`, regularizaciÃ³n.
9. **Despliegue**: exportar con `joblib`, usar en APIs o dashboards.

---

(En la siguiente parte se incluirÃ¡n modelos de ML supervisado y no supervisado, mÃ©tricas y visualizaciÃ³n avanzada.)

---
## ğŸ‘¨â€ğŸ« Conceptos Clave

### ğŸ¯ Variables X (features) e y (target)

- **X**: matriz de caracterÃ­sticas (variables independientes)
- **y**: variable objetivo (lo que queremos predecir)

```python
X = df.drop("target", axis=1)
y = df["target"]
```

### âš ï¸ Overfitting vs. Underfitting

- **Overfitting**: El modelo aprende demasiado bien los datos de entrenamiento y falla al generalizar.
- **Underfitting**: El modelo no logra captar la relaciÃ³n subyacente en los datos.

| Tipo de error    | Causa principal                    | SoluciÃ³n comÃºn                  |
|------------------|------------------------------------|----------------------------------|
| Overfitting       | Modelo demasiado complejo          | RegularizaciÃ³n, mÃ¡s datos       |
| Underfitting      | Modelo demasiado simple            | Modelo mÃ¡s complejo             |

### âš–ï¸ Bias-Variance Tradeoff

- **Bias**: Error por suposiciones incorrectas del modelo.
- **Variance**: Sensibilidad del modelo a pequeÃ±as variaciones en los datos.

Ideal: bajo bias y baja varianza â†’ buen desempeÃ±o generalizado.

---

# ğŸ”¢ Parte 2.5: Preprocesamiento NumÃ©rico y CategÃ³rico

## ğŸ”¢ Transformaciones NumÃ©ricas

### 1. ğŸ¯ Centering (Centrado)
$$
x_{centered} = x - \bar{x}
$$

```python
x_centered = x - np.mean(x)
```

---

### 2. âš–ï¸ Standard Scaler (EstandarizaciÃ³n)
$$
x_{std} = \frac{x - \mu}{\sigma}
$$

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x.reshape(-1, 1))
```

---

### 3. ğŸ“Š Min-Max Scaler
$$
x' = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_minmax = scaler.fit_transform(x.reshape(-1, 1))
```

---

### 4. ğŸ§± Binning (DiscretizaciÃ³n)

```python
x_binned = pd.cut(x, bins=3, labels=["bajo", "medio", "alto"])
```

---

### 5. ğŸ” Transformaciones LogarÃ­tmicas
$$
x' = \log(x + 1)
$$

```python
x_log = np.log1p(x)
```

---

## ğŸ§© CodificaciÃ³n de Variables CategÃ³ricas

### ğŸ”¢ Ordinal Encoding

```python
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder(categories=[['Excellent', 'New', 'Like New', 'Good', 'Fair']])
encoded = encoder.fit_transform(cars['condition'].values.reshape(-1,1))
```

---

### ğŸ·ï¸ Label Encoding

```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
cars['color'] = encoder.fit_transform(cars['color'])
```

---

### ğŸŸ¦ One-Hot Encoding

```python
ohe = pd.get_dummies(cars['color'])
cars = cars.join(ohe)
```

---

### ğŸ”¢ Binary Encoding

```python
from category_encoders import BinaryEncoder
colors = BinaryEncoder(cols=['color']).fit_transform(cars)
```

---

### ğŸ’  Hashing Encoding

```python
from category_encoders import HashingEncoder
encoder = HashingEncoder(cols='color', n_components=5)
hash_results = encoder.fit_transform(cars['color'])
```

---

### ğŸ¯ Target Encoding

```python
from category_encoders import TargetEncoder
encoder = TargetEncoder(cols='color')
encoded = encoder.fit_transform(cars['color'], cars['sellingprice'])
```

---

### â° Encoding de Fechas

```python
cars['saledate'] = pd.to_datetime(cars['saledate'])
cars['month'] = cars['saledate'].dt.month
cars['dayofweek'] = cars['saledate'].dt.dayofweek
cars['yearbuild_sold'] = cars['saledate'].dt.year - cars['year']
```

---

### âœ… ConclusiÃ³n

| TransformaciÃ³n      | Â¿CuÃ¡ndo usarla?                                |
|---------------------|------------------------------------------------|
| Centering           | PCA o modelos lineales                         |
| Standard Scaler     | Modelos sensibles a escala (SVM, regresiÃ³n)    |
| Min-Max Scaler      | Redes neuronales, normalizaciÃ³n entre [0, 1]  |
| Binning             | Modelos de reglas, simplificaciÃ³n              |
| Log Transform       | ReducciÃ³n de asimetrÃ­a                         |

| CodificaciÃ³n         | Â¿CuÃ¡ndo usarla?                               |
|----------------------|-----------------------------------------------|
| Ordinal              | CategorÃ­as ordenadas                          |
| Label                | Nominal simple                                |
| One-Hot              | Nominal sin orden                             |
| Binary               | Muchas categorÃ­as                             |
| Hashing              | Gran volumen, menos interpretabilidad         |
| Target               | RegresiÃ³n, correlaciÃ³n con variable objetivo  |

---
# Parte 3 :ğŸ“Š Tabla Global de Modelos de Machine Learning en Scikit-learn

GuÃ­a de referencia rÃ¡pida organizada por tipo de modelo, con columnas clave para selecciÃ³n, preprocesamiento, interpretaciÃ³n y ajuste.

---

## âœ… ClasificaciÃ³n

| Modelo                  | Clase `scikit-learn`                          | Lineal | Escalado Necesario | Sensible a Outliers | Interpretable | Predice Probabilidades | Soporta Multiclase | HiperparÃ¡metros Clave          |
|-------------------------|-----------------------------------------------|--------|---------------------|---------------------|----------------|------------------------|---------------------|-------------------------------|
| Logistic Regression     | `sklearn.linear_model.LogisticRegression`     | SÃ­     | âœ… SÃ­               | Alta                | âœ… Alta        | âœ… SÃ­                  | âœ… SÃ­               | `C`, `penalty`, `solver`      |
| K-Nearest Neighbors     | `sklearn.neighbors.KNeighborsClassifier`      | No     | âœ… SÃ­               | Alta                | Media          | âŒ No                 | âœ… SÃ­               | `n_neighbors`, `weights`      |
| Decision Tree           | `sklearn.tree.DecisionTreeClassifier`         | No     | âŒ No               | âœ… Baja             | Media          | âŒ No                 | âœ… SÃ­               | `max_depth`, `min_samples_split` |
| Random Forest           | `sklearn.ensemble.RandomForestClassifier`     | No     | âŒ No               | âœ… Baja             | âŒ Baja         | âœ… SÃ­                  | âœ… SÃ­               | `n_estimators`, `max_depth`   |
| Support Vector Machine  | `sklearn.svm.SVC`                              | SÃ­     | âœ… SÃ­               | Alta                | âŒ Baja         | âœ… SÃ­ (`probability`)  | âœ… SÃ­               | `C`, `kernel`, `gamma`        |
| Gaussian Naive Bayes    | `sklearn.naive_bayes.GaussianNB`              | SÃ­     | Recomendado        | Alta                | âœ… Alta        | âœ… SÃ­                  | âœ… SÃ­               | -                             |

---

## ğŸ“ˆ RegresiÃ³n

| Modelo                  | Clase `scikit-learn`                          | Lineal | Escalado Necesario | Sensible a Outliers | Interpretable | Soporta Multioutput | HiperparÃ¡metros Clave           |
|-------------------------|-----------------------------------------------|--------|---------------------|---------------------|----------------|----------------------|-------------------------------|
| Linear Regression       | `sklearn.linear_model.LinearRegression`       | SÃ­     | âœ… SÃ­               | Alta                | âœ… Alta        | âœ… SÃ­                | -                             |
| Ridge                   | `sklearn.linear_model.Ridge`                  | SÃ­     | âœ… SÃ­               | Alta                | âœ… Alta        | âœ… SÃ­                | `alpha`                       |
| Lasso                   | `sklearn.linear_model.Lasso`                  | SÃ­     | âœ… SÃ­               | Alta                | âœ… Alta        | âœ… SÃ­                | `alpha`                       |
| K-Nearest Regressor     | `sklearn.neighbors.KNeighborsRegressor`       | No     | âœ… SÃ­               | Alta                | Media          | âœ… SÃ­                | `n_neighbors`, `weights`      |
| Decision Tree Regressor| `sklearn.tree.DecisionTreeRegressor`          | No     | âŒ No               | âœ… Baja             | Media          | âœ… SÃ­                | `max_depth`, `min_samples_split` |
| Random Forest Regressor| `sklearn.ensemble.RandomForestRegressor`      | No     | âŒ No               | âœ… Baja             | âŒ Baja         | âœ… SÃ­                | `n_estimators`, `max_depth`   |
| Support Vector Regressor| `sklearn.svm.SVR`                             | No     | âœ… SÃ­               | Alta                | âŒ Baja         | âŒ No                | `C`, `kernel`, `epsilon`      |

---

## ğŸ“Š Clustering (No Supervisado)

| Modelo                  | Clase `scikit-learn`                          | Escalado Necesario | Sensible a Outliers | Necesita k | Predice Cluster (`predict`) | MÃ©tricas comunes             |
|-------------------------|-----------------------------------------------|---------------------|---------------------|------------|------------------------------|------------------------------|
| KMeans                  | `sklearn.cluster.KMeans`                      | âœ… SÃ­               | âœ… Alta             | âœ… SÃ­     | âœ… SÃ­                        | Inertia, Silhouette Score    |
| DBSCAN                  | `sklearn.cluster.DBSCAN`                      | âœ… SÃ­               | âœ… Alta             | âŒ No     | âŒ No (`labels_`)            | Silhouette, Davies-Bouldin   |
| Agglomerative Clustering| `sklearn.cluster.AgglomerativeClustering`     | âœ… SÃ­               | âœ… Alta             | âœ… SÃ­     | âŒ No (`labels_`)            | Silhouette, Dendrogram       |
| Gaussian Mixture (EM)   | `sklearn.mixture.GaussianMixture`            | âœ… SÃ­               | Alta                | âœ… SÃ­     | âœ… SÃ­                        | Log-likelihood, AIC, BIC     |

---

## ğŸ§° Herramientas Complementarias

- `Pipeline`: Secuencia de pasos (`StandardScaler` â†’ `Model`).
- `GridSearchCV`: BÃºsqueda exhaustiva de hiperparÃ¡metros.
- `cross_val_score`: ValidaciÃ³n cruzada rÃ¡pida.
- `train_test_split`: Separar `X_train`, `X_test`, `y_train`, `y_test`.
- `StandardScaler`, `MinMaxScaler`: NormalizaciÃ³n / escalado.
- `OneHotEncoder`: Para variables categÃ³ricas.
- `SelectKBest`, `PCA`: SelecciÃ³n / reducciÃ³n de caracterÃ­sticas.

---

> ğŸ§  **Consejo:** Usa `Pipeline` junto con `GridSearchCV` para combinar normalizaciÃ³n, selecciÃ³n de variables y modelo en un solo flujo reproducible.

---

# ğŸ¤– Parte 4: Modelado Supervisado
## ğŸ“ˆ RegresiÃ³n Lineal

Modelo bÃ¡sico con una variable:

$$
\hat{y} = mx + b
$$

### âœ… Supuestos

1. Linealidad
2. Homocedasticidad
3. Independencia de errores
4. Normalidad de los residuos

### ğŸ“Œ Simple Linear Regression

```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([1.5, 2.0, 3.5, 3.7, 4.5])

model = LinearRegression()
model.fit(x, y)

y_pred = model.predict(x)
plt.scatter(x, y, label="Datos reales")
plt.plot(x, y_pred, color="red", label="RegresiÃ³n")
plt.legend()
plt.title("RegresiÃ³n Lineal Simple")
plt.show()
```

---

### ğŸ“Œ Multiple Linear Regression

```python
data = pd.DataFrame({
    "horas_estudio": [1, 2, 3, 4, 5],
    "horas_sueÃ±o": [8, 7, 6, 5, 4],
    "nota": [60, 65, 70, 75, 80]
})
X = data[["horas_estudio", "horas_sueÃ±o"]]
y = data["nota"]

model = LinearRegression()
model.fit(X, y)
print("Coeficientes:", model.coef_)
print("Intercepto:", model.intercept_)
```

---

### ğŸ“Š CorrelaciÃ³n

```python
import seaborn as sns
sns.heatmap(data.corr(), annot=True, cmap="coolwarm")
plt.title("Matriz de CorrelaciÃ³n")
plt.show()
```

---

### ğŸ“ EvaluaciÃ³n del Modelo

* MAE: $\frac{1}{n} \sum |y - \hat{y}|$
- MSE: $\frac{1}{n} \sum (y - \hat{y})^2$
- RMSE: $\sqrt{MSE}$
- RÂ² Score: Varianza explicada por el modelo

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred = model.predict(X)
print("MAE:", mean_absolute_error(y, y_pred))
print("MSE:", mean_squared_error(y, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y, y_pred)))
print("RÂ²:", r2_score(y, y_pred))
```

---

## ğŸ§® OLS vs Gradient Descent

| MÃ©todo              | OLS                          | Gradient Descent (GD)        |
|---------------------|-------------------------------|-------------------------------|
| Tipo                | AnalÃ­tico                     | Iterativo                     |
| Exactitud           | SoluciÃ³n exacta               | AproximaciÃ³n                  |
| Velocidad (pocos datos) | RÃ¡pido                     | MÃ¡s lento                     |
| Velocidad (big data)    | Lento (Ã¡lgebra matricial)  | Escalable                     |
| Requiere ajuste     | No                            | SÃ­ (learning rate, epochs)    |

### OLS FÃ³rmula:

$$
\beta = (X^T X)^{-1} X^T y
$$

### GD FÃ³rmulas:

$$
m \leftarrow m - \eta \cdot \frac{\partial}{\partial m} \text{Loss},\quad
b \leftarrow b - \eta \cdot \frac{\partial}{\partial b} \text{Loss}
$$

---

## ğŸ” RegresiÃ³n LogÃ­stica

### ğŸ“˜ DefiniciÃ³n

Predice una variable categÃ³rica (binaria) usando una transformaciÃ³n sigmoide.
$$
P(y = 1 | x) = \frac{1}{1 + e^{-(b + m_1 x_1 + \dots + m_n x_n)}}
$$

### ğŸ§ª ImplementaciÃ³n

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = LogisticRegression(class_weight='balanced')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

### ğŸ“Š MÃ©tricas y Matriz de ConfusiÃ³n

- Accuracy
- Precision, Recall
- F1 Score
- ROC-AUC

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

> âš ï¸ Usa `predict_proba` para obtener probabilidades y ajustar el threshold si es necesario.
---

### ğŸ“ˆ ROC Curve & AUC

```python
from sklearn.metrics import roc_curve, roc_auc_score

y_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], '--', color='gray')
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC Curve")
plt.legend()
plt.show()
```

---

### âš™ï¸ Ajuste de Threshold

```python
y_pred_custom = (y_proba > 0.3).astype(int)
```

---

## ğŸ§  Consideraciones Avanzadas

- Usa `class_weight="balanced"` si hay desbalance.
- Revisa correlaciones para evitar multicolinealidad.
- Normaliza si las variables tienen escalas diferentes.
- Visualiza con curvas ROC y matriz de confusiÃ³n.

---

## ğŸ” K-Nearest Neighbors (KNN)

### ğŸ§  Idea Principal

KNN predice un resultado **basado en los K vecinos mÃ¡s cercanos** en el espacio de caracterÃ­sticas.

- **KNN ClasificaciÃ³n**: vota la clase mÃ¡s comÃºn entre los vecinos ( 0 o 1).
- **KNN RegresiÃ³n**: promedia el valor de salida de los vecinos( valores numÃ©ricos como estrellas  de una pelÃ­cula) .

### ğŸ“ Recomendaciones generales

- Siempre **escalar los datos** (StandardScaler o MinMaxScaler) antes de usar KNN.
- Seleccionar el valor Ã³ptimo de K es **crÃ­tico** (ni muy bajo ni muy alto).
- El parÃ¡metro `weights` puede cambiar significativamente el resultado:
  - `'uniform'`: todos los vecinos tienen el mismo peso.
  - `'distance'`: los vecinos mÃ¡s cercanos tienen mÃ¡s influencia.

---

### ğŸ“˜ KNN para ClasificaciÃ³n

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# Ajustar K automÃ¡ticamente
param_grid = {'n_neighbors': range(1, 21), 'weights': ['uniform', 'distance']}
grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)

print("Mejor K:", grid.best_params_['n_neighbors'])
print("Pesos usados:", grid.best_params_['weights'])

# Evaluar
y_pred = grid.predict(X_test)
print(classification_report(y_test, y_pred))
```

---

### ğŸ“˜ KNN para RegresiÃ³n

```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

reg = KNeighborsRegressor(n_neighbors=5, weights='distance')
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
```

---

### ğŸ§ª ComparaciÃ³n de pesos en regresiÃ³n

```python
for weight in ['uniform', 'distance']:
    model = KNeighborsRegressor(n_neighbors=5, weights=weight)
    model.fit(X_train, y_train)
    print(f"{weight} MSE:", mean_squared_error(y_test, model.predict(X_test)))
```

---

### ğŸ§  ElecciÃ³n de K

- K muy pequeÃ±o â†’ sobreajuste (modelo muy flexible).
- K muy grande â†’ subajuste (modelo demasiado general).
- Se recomienda probar con `GridSearchCV` o validaciÃ³n cruzada.

---

## ğŸŒ³ Ãrboles de DecisiÃ³n (Decision Trees)

### ğŸ§  Idea Principal

Dividen el espacio de decisiones en **reglas tipo sÃ­/no** segÃºn las caracterÃ­sticas mÃ¡s importantes.

- Se crean nodos que maximizan la **ganancia de informaciÃ³n** o reducen la **impureza de Gini**.

---

### ğŸ“˜ Criterios de divisiÃ³n

- **Gini Impurity**:
  $$
  Gini = 1 - \sum_{i=1}^C p_i^2
  $$

- **Entropy / Information Gain**:
  $$
  Entropy = - \sum_{i=1}^C p_i \log_2 p_i
  $$

  $$
  IG = Entropy_{parent} - \sum \left( \frac{n_{child}}{n_{total}} \cdot Entropy_{child} \right)
  $$

---

### ğŸ’» ImplementaciÃ³n

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

model = DecisionTreeClassifier(criterion='gini', max_depth=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
plot_tree(model, filled=True, feature_names=X.columns)
plt.show()
```

---

### âš™ï¸ HiperparÃ¡metros clave

- `criterion`: `'gini'` o `'entropy'`
- `max_depth`: profundidad mÃ¡xima del Ã¡rbol
- `min_samples_split`: mÃ­nimo de muestras para dividir un nodo
- `min_samples_leaf`: mÃ­nimo de muestras en una hoja

---

### âœ… Ventajas y Desventajas

| Pros                           | Contras                             |
|--------------------------------|--------------------------------------|
| FÃ¡cil de interpretar           | Tienden al sobreajuste               |
| No necesita normalizaciÃ³n      | Sensibles a pequeÃ±os cambios         |
| Acepta variables categÃ³ricas   | Ãrboles muy grandes son difÃ­ciles de manejar |

---

> ğŸŒ± Para mejorar los Ã¡rboles individuales, se usan **Random Forests** y **Boosting**, que se explicarÃ¡n mÃ¡s adelante.

