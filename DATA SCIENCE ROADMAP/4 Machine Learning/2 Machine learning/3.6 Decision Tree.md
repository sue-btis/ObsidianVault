#Machine_Learning 
# ğŸŒ³ Decision Trees

Decision Trees are intuitive, tree-like models used for both **classification** and **regression** tasks. They work by asking a sequence of questions that split the data based on the features to reach a decision.

---

## ğŸ§  Core Concepts

### ğŸ”¹ What is a Decision Tree?

A **Decision Tree** breaks down a dataset into smaller subsets while an associated tree is incrementally developed. The result is a tree with decision nodes and leaf nodes:

- **Decision Node**: Splits the data based on a feature.
- **Leaf Node**: Represents a final decision or prediction.

---

## âœ‚ï¸ Splitting Criteria

To decide how to split the nodes, decision trees use metrics that measure **impurity** or **disorder** in a dataset.

---

## 1. âš–ï¸ Gini Impurity

Gini Impurity measures how often a randomly chosen element would be incorrectly labeled.

### ğŸ§® Formula:

For a node with classes $p_1, p_2, \dots, p_n$:

$$
Gini = 1 - \sum_{i=1}^n p_i^2
$$

- Minimum Gini = 0 (pure node: only one class)
- Maximum Gini = when classes are evenly mixed

### ğŸ“Œ Example:
If a node has 70% class A and 30% class B:

$$
Gini = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42
$$

---

## 2. ğŸ“Š Information Gain (Entropy)

**Entropy** measures the unpredictability in a dataset. The higher the entropy, the more mixed the data.

### ğŸ§® Entropy Formula:

$$
Entropy = -\sum_{i=1}^n p_i \log_2(p_i)
$$

### ğŸ“ˆ Information Gain:

Information Gain tells us how much entropy is reduced by a split:

$$
IG = Entropy_{parent} - \sum_{i=1}^k \frac{N_i}{N} \cdot Entropy_{i}
$$

Where:
- $N_i$: samples in child node
- $N$: total samples

---

## ğŸ¤– Using Decision Trees in Scikit-Learn

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = DecisionTreeClassifier(criterion="gini", max_depth=3)
model.fit(X_train, y_train)

# Visualize
plt.figure(figsize=(12,8))
plot_tree(model, filled=True, feature_names=["sepal length", "sepal width", "petal length", "petal width"])
plt.show()

# Score
print("Accuracy:", model.score(X_test, y_test))
```

---

## ğŸ§¾ Pros & Cons

âœ… Easy to interpret  
âœ… Works with both numerical & categorical features  
âš ï¸ Prone to overfitting  
âš ï¸ Sensitive to small changes in data  

---

## ğŸ§ª Best Practices

- Use **pruning** to avoid overfitting
- Use **cross-validation** for better performance
- Consider ensemble methods like **Random Forests** for higher accuracy

---

## ğŸ“š Summary Table

| Concept | Formula | Goal |
|--------|---------|------|
| Gini Impurity | $1 - \sum p_i^2$ | Measure impurity |
| Entropy | $-\sum p_i \log_2 p_i$ | Measure disorder |
| Info Gain | $Entropy_{parent} - \sum \frac{N_i}{N} Entropy_i$ | Measure improvement after split |
