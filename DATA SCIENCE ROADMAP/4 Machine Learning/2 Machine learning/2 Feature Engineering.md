#Machine_Learning 

[[2.1 NUM AND CATEG TRANSFORMATIONS]]
[[2.2 DIMENSIONAL TRANSFORMATIONS]]
[[2.3 METHODS FOR FEATURE REDUCTION]]




<div style="text-align: center;">
  <img src="999. IMG FOLDER/image-25.png" alt="Mi Imagen" width="600">
</div>

# ðŸ› ï¸ IntroducciÃ³n a Feature Engineering

## ðŸ“Œ Â¿QuÃ© es Feature Engineering?

Feature Engineering es el proceso de preparar, transformar, seleccionar o crear variables (llamadas _features_) para mejorar el desempeÃ±o de un modelo de aprendizaje automÃ¡tico.

Un _feature_ es cualquier propiedad medible que puede servir como entrada para un modelo. Por ejemplo:

- Temperatura, humedad, altitud para predecir precipitaciones.
    
El proceso de Feature Engineering se vuelve esencial cuando:

- No sabemos quÃ© variables usar.
    
- Hay demasiadas variables.
    
- Algunas estÃ¡n mal formateadas o son redundantes.

## ðŸŽ¯ Â¿Por quÃ© necesitamos Feature Engineering?

Para asegurar que un modelo sea:

1. **Eficaz (Performance)**: Que prediga bien.
    
2. **RÃ¡pido (Runtime)**: Que no sea lento en producciÃ³n.
    
3. **Interpretativo (Interpretability)**: Que dÃ© informaciÃ³n Ãºtil y comprensible.
    
4. **Generalizable (Generalizability)**: Que funcione con nuevos datos.

---

## ðŸ”„ Â¿DÃ³nde encaja en el flujo de Machine Learning?

Aunque se enseÃ±a como un paso intermedio entre EDA y modelado, en la prÃ¡ctica **Feature Engineering ocurre antes, durante y despuÃ©s del modelado**.

Se divide en tres grandes categorÃ­as:

### 1. ðŸ”§ MÃ©todos de TransformaciÃ³n

Aplican transformaciones a las variables para hacerlas mÃ¡s aptas para los modelos.

Ejemplos:

- NormalizaciÃ³n / Escalamiento
    
- One-hot encoding
    
- Binning
    
- Transformaciones logarÃ­tmicas
    
- Hashing
    

**Objetivo**: mejorar desempeÃ±o, velocidad y comprensiÃ³n.

---

### 2. ðŸ“‰ MÃ©todos de ReducciÃ³n de Dimensionalidad

Reducen el nÃºmero de _features_ conservando (o mejorando) el rendimiento.

Ejemplos:

- PCA (AnÃ¡lisis de Componentes Principales)
    
- LDA (AnÃ¡lisis Discriminante Lineal)
    

Estas tÃ©cnicas generan nuevos _features_ matemÃ¡ticos no directamente interpretables pero muy eficientes computacionalmente.

**TambiÃ©n se llaman mÃ©todos de ExtracciÃ³n de CaracterÃ­sticas.**

---

### 3. ðŸŽ¯ MÃ©todos de SelecciÃ³n de CaracterÃ­sticas

Permiten elegir cuÃ¡les _features_ usar directamente.

#### i. Filter Methods

Usan estadÃ­sticas simples, sin modelo:

- CorrelaciÃ³n (Pearson, Spearman)
    
- Chi-cuadrado ($\chi^2$)
    
- ANOVA
    
- InformaciÃ³n mutua
    

#### ii. Wrapper Methods

Usan modelos para probar subconjuntos de variables:

- Forward Selection
    
- Backward Elimination
    
- Sequential Floating
    

#### iii. Embedded Methods

Seleccionan _features_ durante el entrenamiento del modelo:

- RegularizaciÃ³n (Lasso, Ridge)
    
- Importancia de variables en Ã¡rboles (Random Forest, XGBoost)
    

---

## ðŸ§  Resumen RÃ¡pido

|Tipo de TÃ©cnica|Mejora|Â¿CuÃ¡ndo se usa?|
|---|---|---|
|TransformaciÃ³n de Features|Performance, runtime, interpretaciÃ³n|Antes del modelo|
|ReducciÃ³n de Dimensionalidad|Performance, runtime|Antes/durante el modelado|
|SelecciÃ³n de CaracterÃ­sticas|Interpretabilidad, generalizaciÃ³n|Antes o durante el modelado|

> ðŸ“¦ _Feature Engineering es la etapa donde la intuiciÃ³n humana y la tÃ©cnica se combinan para construir modelos robustos y eficientes._

