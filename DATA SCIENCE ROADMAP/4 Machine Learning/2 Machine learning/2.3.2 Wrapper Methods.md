#Machine_Learning 
# üîç Wrapper Methods (Simplified)

Wrapper methods evaluate subsets of features based on model performance. They train a model multiple times using different subsets and keep the one that performs best.

---

## 1Ô∏è‚É£ Sequential Forward Selection (SFS)
- **Starts with no features**
- Adds one feature at a time based on which improves the model most.

```python
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt

# Load data
X, y = load_iris(return_X_y=True)
X = StandardScaler().fit_transform(X)

# Model
model = LogisticRegression(max_iter=1000)

# SFS
sfs = SFS(model, k_features=2, forward=True, floating=False, scoring='accuracy', cv=5)
sfs.fit(X, y)

# Subsets selected
print(sfs.subsets_)

# Plot results
plot_sfs(sfs.get_metric_dict())
plt.title("Sequential Forward Selection")
plt.grid()
plt.show()
```

---

## 2Ô∏è‚É£ Sequential Backward Selection (SBS)
- **Starts with all features**
- Removes one feature at a time

```python
sbs = SFS(model, k_features=2, forward=False, floating=False, scoring='accuracy', cv=5)
sbs.fit(X, y)

print(sbs.subsets_)
plot_sfs(sbs.get_metric_dict())
plt.title("Sequential Backward Selection")
plt.grid()
plt.show()
```

---

## 3Ô∏è‚É£ Sequential Forward Floating Selection (SFFS)
- Like SFS but with flexibility: can **add and remove features** dynamically

```python
sffs = SFS(model, k_features=2, forward=True, floating=True, scoring='accuracy', cv=5)
sffs.fit(X, y)

print(sffs.subsets_)
plot_sfs(sffs.get_metric_dict())
plt.title("Sequential Forward Floating Selection")
plt.grid()
plt.show()
```

---

## 4Ô∏è‚É£ Sequential Backward Floating Selection (SBFS)
- Like SBS but also allows adding a feature back after removing it

```python
sbfs = SFS(model, k_features=2, forward=False, floating=True, scoring='accuracy', cv=5)
sbfs.fit(X, y)

print(sbfs.subsets_)
plot_sfs(sbfs.get_metric_dict())
plt.title("Sequential Backward Floating Selection")
plt.grid()
plt.show()
```

---

## 5Ô∏è‚É£ Recursive Feature Elimination (RFE)

Eliminates the weakest feature(s) iteratively based on model coefficients.

```python
from sklearn.feature_selection import RFE

rfe = RFE(estimator=model, n_features_to_select=2)
rfe.fit(X, y)

print("Selected Features (mask):", rfe.support_)
print("Feature Ranking:", rfe.ranking_)
```

You can extract feature names using:

```python
feature_names = ['feature1', 'feature2', 'feature3', 'feature4']
selected = [f for f, s in zip(feature_names, rfe.support_) if s]
print("Chosen features:", selected)
```

---

## ‚úÖ Summary

| Method | Starts With | Strategy | Allows Floating | Good For |
|--------|-------------|----------|-----------------|-----------|
| SFS    | No features | Add best | ‚ùå              | Small to medium datasets |
| SBS    | All features| Remove worst | ‚ùå         | Feature pruning |
| SFFS   | No features | Add/Remove | ‚úÖ            | Flexible search |
| SBFS   | All features| Remove/Add | ‚úÖ           | Flexible pruning |
| RFE    | All features| Remove recursively | ‚ùå     | Model-driven selection |

---

üß† Use **wrapper methods** when:
- You have enough computational power
- You want **model-specific** feature selection
- Accuracy is more important than speed
