#Machine_Learning 

# 🧠 Principal Component Analysis (PCA)

## 📌 What is PCA?

**Principal Component Analysis (PCA)** is a technique used for **dimensionality reduction**. It transforms high-dimensional data into a lower-dimensional space while preserving as much **variance** as possible.

> 🎯 Ideal when you want to simplify data while keeping its structure intact.

---

## 🎯 Why and When to Use PCA

Use PCA when:
- Your dataset has **many correlated features**
- You want to **speed up** model training
- You want to **reduce overfitting**
- You need a way to **visualize high-dimensional data**

### Benefits
- ✅ Reduces computational cost
- ✅ Removes noise and redundancy
- ✅ Improves generalization
- ✅ Enhances visualization

---

## ⚙️ How PCA Works (Step by Step)

1. **Standardize** the features
2. **Compute the covariance matrix**
3. **Extract eigenvectors and eigenvalues**
4. **Select top `k` components** with the highest eigenvalues
5. **Project the data** onto these components

> 💡 The directions of maximum variance become your new features (called *principal components*).

---

## 🧪 Implementing PCA in Python

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(pca.explained_variance_ratio_)
```

---

## 📉 How Many Components Should I Keep?

### 📈 Explained Variance

Plot the **cumulative explained variance** to decide how many components to retain:

```python
import matplotlib.pyplot as plt

pca = PCA().fit(X_scaled)
cumulative = pca.explained_variance_ratio_.cumsum()

plt.plot(range(1, len(cumulative)+1), cumulative, marker='o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance vs Number of Components")
plt.legend()
plt.grid()
plt.show()
```

> ✅ A good rule: choose the smallest number of components that explain **90–95%** of the variance.

---

## 🔬 Evaluating PCA: Does It Work?

### 1. Compare Model Performance (With vs Without PCA)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# Without PCA
scores_base = cross_val_score(LogisticRegression(), X_scaled, y, cv=5)

# With PCA
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_scaled)
scores_pca = cross_val_score(LogisticRegression(), X_reduced, y, cv=5)

print(f"Accuracy without PCA: {scores_base.mean():.2f}")
print(f"Accuracy with PCA: {scores_pca.mean():.2f}")
```

> 🎯 If accuracy is similar or better, PCA is effective.

---

### 2. Visualize the Projection

```python
pca = PCA(n_components=2)
X_2D = pca.fit_transform(X_scaled)

plt.scatter(X_2D[:, 0], X_2D[:, 1], c=y, cmap='viridis')
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("2D PCA Projection")
plt.colorbar()
plt.show()
```

> Helpful to detect **clusters** or **patterns** in the data.

---

## 🖼️ PCA for Image Compression

Images are high-dimensional (many pixels). PCA is useful for:
- 📦 **Compression**
- 🎨 **Noise reduction**
- 🔍 **Feature extraction**

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

digits = load_digits()
image = digits.data[0]

# Original image
plt.imshow(image.reshape(8, 8), cmap='gray')
plt.title("Original")
plt.show()

# Compressed with PCA
pca = PCA(n_components=10)
compressed = pca.fit_transform([image])
reconstructed = pca.inverse_transform(compressed)

plt.imshow(reconstructed.reshape(8, 8), cmap='gray')
plt.title("Reconstructed")
plt.show()
```

