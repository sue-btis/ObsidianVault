#Machine_Learning 

# ğŸ§  Principal Component Analysis (PCA)

## ğŸ“Œ What is PCA?

**Principal Component Analysis (PCA)** is a technique used for **dimensionality reduction**. It transforms high-dimensional data into a lower-dimensional space while preserving as much **variance** as possible.

> ğŸ¯ Ideal when you want to simplify data while keeping its structure intact.

---

## ğŸ¯ Why and When to Use PCA

Use PCA when:
- Your dataset has **many correlated features**
- You want to **speed up** model training
- You want to **reduce overfitting**
- You need a way to **visualize high-dimensional data**

### Benefits
- âœ… Reduces computational cost
- âœ… Removes noise and redundancy
- âœ… Improves generalization
- âœ… Enhances visualization

---

## âš™ï¸ How PCA Works (Step by Step)

1. **Standardize** the features
2. **Compute the covariance matrix**
3. **Extract eigenvectors and eigenvalues**
4. **Select top `k` components** with the highest eigenvalues
5. **Project the data** onto these components

> ğŸ’¡ The directions of maximum variance become your new features (called *principal components*).

---

## ğŸ§ª Implementing PCA in Python

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(pca.explained_variance_ratio_)
```

---

## ğŸ“‰ How Many Components Should I Keep?

### ğŸ“ˆ Explained Variance

Plot the **cumulative explained variance** to decide how many components to retain:

```python
import matplotlib.pyplot as plt

pca = PCA().fit(X_scaled)
cumulative = pca.explained_variance_ratio_.cumsum()

plt.plot(range(1, len(cumulative)+1), cumulative, marker='o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance vs Number of Components")
plt.legend()
plt.grid()
plt.show()
```

> âœ… A good rule: choose the smallest number of components that explain **90â€“95%** of the variance.

---

## ğŸ”¬ Evaluating PCA: Does It Work?

### 1. Compare Model Performance (With vs Without PCA)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# Without PCA
scores_base = cross_val_score(LogisticRegression(), X_scaled, y, cv=5)

# With PCA
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_scaled)
scores_pca = cross_val_score(LogisticRegression(), X_reduced, y, cv=5)

print(f"Accuracy without PCA: {scores_base.mean():.2f}")
print(f"Accuracy with PCA: {scores_pca.mean():.2f}")
```

> ğŸ¯ If accuracy is similar or better, PCA is effective.

---

### 2. Visualize the Projection

```python
pca = PCA(n_components=2)
X_2D = pca.fit_transform(X_scaled)

plt.scatter(X_2D[:, 0], X_2D[:, 1], c=y, cmap='viridis')
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("2D PCA Projection")
plt.colorbar()
plt.show()
```

> Helpful to detect **clusters** or **patterns** in the data.

---

## ğŸ–¼ï¸ PCA for Image Compression

Images are high-dimensional (many pixels). PCA is useful for:
- ğŸ“¦ **Compression**
- ğŸ¨ **Noise reduction**
- ğŸ” **Feature extraction**

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

digits = load_digits()
image = digits.data[0]

# Original image
plt.imshow(image.reshape(8, 8), cmap='gray')
plt.title("Original")
plt.show()

# Compressed with PCA
pca = PCA(n_components=10)
compressed = pca.fit_transform([image])
reconstructed = pca.inverse_transform(compressed)

plt.imshow(reconstructed.reshape(8, 8), cmap='gray')
plt.title("Reconstructed")
plt.show()
```

