#Machine_Learning 

# 🧩 Encoding Categorical Variables in Python

>**Muchos modelos de machine learning requieren datos numéricos, por lo que las variables categóricas deben transformarse**. 

## 📚 Tipos de Variables Categóricas

- **Nominales**: sin orden lógico (ej. color, estado).
- **Ordinales**: con jerarquía (ej. condición de un producto: Nuevo > Bueno > Regular).

---

## 1. 🔢 Ordinal Encoding

### ✔️ ¿Cuándo usarlo?
Cuando la variable tiene **orden**, como la condición de un auto.

### 🧪 Ejemplo

```python
rating_dict = {'Excellent':5, 'New':4, 'Like New':3, 'Good':2, 'Fair':1}
cars['condition_rating'] = cars['condition'].map(rating_dict)
```

### 📦 Con `OrdinalEncoder` de sklearn

```python
from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder(categories=[['Excellent', 'New', 'Like New', 'Good', 'Fair']])
cars['condition_rating'] = encoder.fit_transform(cars['condition'].values.reshape(-1,1))
```

#### Antes/despues
**Antes:**

| condition |
|-----------|
| Excellent |
| Good      |
| Fair      |

**Después:**

| condition | condition_rating |
|-----------|------------------|
| Excellent | 5                |
| Good      | 2                |
| Fair      | 1                |

---



## 2. 🏷️ Label Encoding

### ✔️ ¿Cuándo usarlo?
Para datos nominales con **muchos valores únicos**, pero **no hay orden**.

```python
# Convertir a categoría
cars['color'] = cars['color'].astype('category')
cars['color'] = cars['color'].cat.codes
```

### 📦 Con `LabelEncoder` de sklearn

```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
cars['color'] = encoder.fit_transform(cars['color'])
```

⚠️ Puede inducir a modelos a pensar que hay orden entre categorías.

#### Antes/despues
**Antes:**

| color  |
|--------|
| black  |
| white  |
| blue   |

**Después:**

| color  |
|--------|
| 2      |
| 18     |
| 3      |

---



## 3. 🟦 One-Hot Encoding

### ✔️ ¿Cuándo usarlo?
Para **evitar supuesta jerarquía** en datos nominales.

```python
ohe = pd.get_dummies(cars['color'])
cars = cars.join(ohe)
```

✅ Evita malentendidos del modelo.  
⚠️ Puede aumentar mucho la dimensionalidad.

#### Antes/despues
**Antes:**

| color  |
|--------|
| black  |
| white  |
| blue   |

**Después:**

| black | white | blue |
|-------|-------|------|
| 1     | 0     | 0    |
| 0     | 1     | 0    |
| 0     | 0     | 1    |

---



## 4. 🔢 Binary Encoding

### ✔️ ¿Cuándo usarlo?
Cuando hay **muchas categorías nominales** y queremos **reducir dimensionalidad**.

```python
from category_encoders import BinaryEncoder

colors = BinaryEncoder(cols=['color'], drop_invariant=True).fit_transform(cars)
```

👍 Reduce número de columnas  
⚠️ No interpretable fácilmente

#### Antes/despues
**Antes:**

| color  |
|--------|
| black  |
| white  |
| blue   |

**Después (5 bits):**

| color_0 | color_1 | color_2 | color_3 | color_4 |
|---------|---------|---------|---------|---------|
| 1       | 0       | 0       | 1       | 0       |
| 0       | 1       | 0       | 0       | 1       |
| 1       | 1       | 0       | 0       | 0       |

---



## 5. 💠 Hashing Encoding

### ✔️ ¿Cuándo usarlo?
Cuando importa el **rendimiento** más que la **interpretabilidad**.

```python
from category_encoders import HashingEncoder

encoder = HashingEncoder(cols='color', n_components=5)
hash_results = encoder.fit_transform(cars['color'])
```

⚠️ Puede haber colisiones = pérdida de información

#### Antes/despues
**Antes:**

| color  |
|--------|
| black  |
| white  |
| blue   |

**Después (hash = 5 columnas):**

| col0 | col1 | col2 | col3 | col4 |
|------|------|------|------|------|
| 0    | 1    | 0    | 1    | 1    |
| 1    | 0    | 1    | 0    | 1    |
| 1    | 1    | 1    | 0    | 0    |


---



## 6. 🎯 Target Encoding

### ✔️ ¿Cuándo usarlo?
En tareas **de regresión supervisada**. Usa el promedio del target para cada categoría.

```python
from category_encoders import TargetEncoder

encoder = TargetEncoder(cols='color')
encoder_results = encoder.fit_transform(cars['color'], cars['sellingprice'])
```

⚠️ Riesgo de **sobreajuste**

#### Antes/despues
**Antes:**

| color | sellingprice |
|-------|--------------|
| black | 15000        |
| white | 18000        |
| blue  | 10000        |

**Después:**

| color (encoded) |
|-----------------|
| 14769.29        |
| 18048.52        |
| 8458.25         |


---

## ⏰ Encoding de Variables de Fecha y Hora

```python
cars['saledate'] = pd.to_datetime(cars['saledate'])

cars['month'] = cars['saledate'].dt.month
cars['dayofweek'] = cars['saledate'].dt.dayofweek
cars['yearbuild_sold'] = cars['saledate'].dt.year - cars['year']
```

#### Antes/despues
**Antes:**

| saledate           |
|--------------------|
| 2023-05-12 00:00:00|

**Después:**

| month | dayofweek | yearbuild_sold |
|-------|-----------|----------------|
| 5     | 4         | 1              |

---


### Resumen 

| Método              | ¿Cuándo usarlo?                       | Pros                         | Contras                       |
|---------------------|----------------------------------------|------------------------------|-------------------------------|
| Ordinal Encoding     | Variables ordenadas                  | Simple, interpretable        | Puede inducir sesgos          |
| Label Encoding       | Nominales simples                    | Fácil de implementar         | Puede inducir orden falso     |
| One-Hot Encoding     | Nominales sin orden                  | Evita jerarquía falsa        | Incrementa dimensionalidad    |
| Binary Encoding      | Muchas categorías                    | Eficiente                    | Menos interpretable           |
| Hashing Encoding     | Modelos rápidos, menos interpretación | Muy eficiente                | Colisiones                    |
| Target Encoding      | Regresión, correlación con target     | Captura relación directa     | Sobreajuste                   |
