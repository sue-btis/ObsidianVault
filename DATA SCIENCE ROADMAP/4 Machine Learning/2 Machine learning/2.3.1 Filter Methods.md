#Machine_Learning
# ðŸ§  Feature Engineering: Filter Methods

Filter methods are a category of feature selection techniques used **before** training a machine learning model. These methods evaluate features based on **statistical characteristics**, independently of any model.

---

## ðŸ“Š 1. Variance Threshold

### ðŸ§¾ What is it?
If a feature (column) has little to no variation (e.g., the same value almost everywhere), it wonâ€™t help the model learn.

### ðŸ’¡ Intuition:
Imagine a column with the same value for every row. It doesnâ€™t help in separating or predicting anything!

### Formula:
$$
Var(X) = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2
$$

### When to use?
- When you want to **remove constant or nearly constant** features.
- Useful in preprocessing steps for high-dimensional data (e.g., text vectors).

### Python Example:

```python
from sklearn.feature_selection import VarianceThreshold
import numpy as np

X = np.array([[0, 2, 0, 3],
              [0, 1, 4, 3],
              [0, 1, 1, 3]])

# Remove features with low variance
sel = VarianceThreshold(threshold=0.5)
X_reduced = sel.fit_transform(X)

print(X_reduced)
```

---

## ðŸ”— 2. Pearsonâ€™s Correlation

### ðŸ§¾ What is it?
Measures how strongly two variables are **linearly related**.

- Value ranges from -1 to +1:
  - +1 = strong positive correlation
  - 0 = no correlation
  - -1 = strong negative correlation

### ðŸ’¡ Intuition:
If two features are highly correlated, they carry similar info. We might keep one and drop the other.

### Formula:
$$
\rho_{X,Y} = \frac{cov(X, Y)}{\sigma_X \cdot \sigma_Y}
$$

- Values close to +1 or -1 indicate strong correlation.
- Near 0 = weak or no linear relationship.

### When to use?
- Useful for continuous features and target variables.

### Python Example:

```python
import pandas as pd

df = pd.DataFrame({
    "X1": [1, 2, 3, 4, 5],
    "X2": [2, 1, 4, 3, 5],
    "y":  [1, 2, 3, 4, 5]
})

correlations = df.corr()["y"].drop("y")
print(correlations)
```

---

## ðŸ§© 3. Mutual Information

### ðŸ§¾ What is it?
Measures how much **knowing one variable tells you about another**. Unlike correlation, it works for **any kind of relationship**, not just linear.

### ðŸ’¡ Intuition:
If changing feature A gives a lot of info about the target, then A has high mutual information.
### Formula (Discrete version):
$$
I(X;Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y) \log \left( \frac{p(x,y)}{p(x)p(y)} \right)
$$

### When to use?
- Works for **categorical and numerical** variables.
- Robust to non-linear relationships.

### Python Example:

```python
from sklearn.feature_selection import mutual_info_classif
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
mi = mutual_info_classif(X, y)

for i, score in enumerate(mi):
    print(f"Feature {i}: MI Score = {score:.4f}")
```
