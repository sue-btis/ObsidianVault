#Machine_Learning 
#Supervised
#Ensemble_model
# 🧠 Stacking: Ensemble Learning

## 📌 What is Stacking?

**Stacking** is an ensemble learning technique that combines predictions from **multiple base models** (which may be strong or weak learners) using a **final model** (meta-learner) that learns how to best combine their outputs.

> 🎯 Stacking leverages the strengths of different models to improve overall performance.

---

## 🔁 How Stacking Differs from Bagging and Boosting

| Feature               | Bagging (Random Forest) | Boosting (AdaBoost/GBM) | Stacking                   |
|------------------------|--------------------------|--------------------------|----------------------------|
| Learners               | Usually weak             | Weak                    | Strong or weak            |
| Training style         | Parallel                 | Sequential               | Parallel base, sequential meta |
| Data subsampling       | Yes                      | No                       | No                         |
| Combiner               | Voting/Averaging         | Gradient update          | Learner trained on predictions |

---

## ⚙️ How Stacking Works

1. Train multiple **base models** (level 0) on the full training set.
2. Use **K-Fold Cross Validation** to generate **predictions** from base models.
3. **Augment features** with predictions as new inputs.
4. Train a **meta-model** (level 1) using these new features.

---

## 🧪 Example: Using scikit-learn

```python
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv('water_potability.csv')
X = df.drop('Potability', axis=1)
y = df['Potability']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Base models
base_estimators = {
    "logreg": LogisticRegression(random_state=42, max_iter=1000),
    "forest": RandomForestClassifier(random_state=42)
}

# Meta-model
final_estimator = RandomForestClassifier(random_state=42)

# K-Fold for internal prediction
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Stacking model
stacking_model = StackingClassifier(
    estimators=list(base_estimators.items()),
    final_estimator=final_estimator,
    passthrough=True,
    cv=cv,
    stack_method="predict_proba"
)

# Train and evaluate
stacking_model.fit(X_train, y_train)
y_pred = stacking_model.predict(X_test)
print("Stacking Accuracy:", accuracy_score(y_test, y_pred))
```

---

## 🧱 Feature Augmentation

Each sample gets **extra features** from base model predictions.

- If your dataset has `n` features and you use `m` base models,
- The meta-model will learn from `n + m` features.

---

## 📊 Comparing Performances

```python
# Individual models
logreg = LogisticRegression(random_state=42).fit(X_train, y_train)
forest = RandomForestClassifier(random_state=42).fit(X_train, y_train)

print("Logistic Regression Accuracy:", accuracy_score(y_test, logreg.predict(X_test)))
print("Random Forest Accuracy:", accuracy_score(y_test, forest.predict(X_test)))
print("Stacking Accuracy:", accuracy_score(y_test, stacking_model.predict(X_test)))
```

---

## ⚠️ Limitations

- ⚙️ **Computationally expensive**
- 🔢 Needs **large training data** for good results
- 📈 Marginal improvements over best individual model
- 🔄 Careful **hyperparameter tuning** and validation needed

---

## 💡 Tips & Advanced Stacking

- You can **stack multiple layers** (multi-tier stacking)
- Try **diverse models** to increase generalization
- Use **stack_method="predict_proba"** for probabilistic stacking
- Try `StackingRegressor` for regression problems
