#DATA_ANALYSIS
#DS_WorkFlow 
[[3.3 EDA Detailed Guide]]
# ❓ Handling Missing Data

## 🌟 Introduction

Missing data is a common issue in real-world datasets. Understanding the nature of missingness and choosing the right technique for dealing with it is critical to ensuring reliable analyses and models.

---

## 🔍 Types of Missing Data

1. **Structurally Missing**: Missing by design or logical reason (e.g., gender in a survey for single-gender schools).
    
2. **Missing Completely at Random (MCAR)**: Every data point has the same chance of being missing.
    
3. **Missing at Random (MAR)**: Missingness depends only on observed variables.
    
4. **Missing Not at Random (MNAR)**: Missingness depends on unobserved or hidden variables (e.g., skipped medical questions due to embarrassment).
    

---

## 🗑️ When Is It Safe to Use Deletion?

**Safe if:**

- Data is MCAR or MAR **and**
    
- The percentage of missing values is small **and**
    
- The missing variable is weakly correlated with other features
    

**Not safe if:**

- Data is MNAR
    
- A large portion of the dataset would be lost
    

### Deletion Methods

- **Listwise Deletion**: Removes entire row if _any_ value is missing.
    
- **Pairwise Deletion**: Removes row _only_ when a specific variable is needed for computation.
    

---

## 🧩 Single Imputation

Replace missing values with a single estimate.

### 🕒 Time Series: LOCF and NOCB

#### LOCF (Last Observation Carried Forward)

```
df['column'].ffill(inplace=True)
```

Use when past values are representative of missing ones.

#### NOCB (Next Observation Carried Backward)

```
df['column'].bfill(inplace=True)
```

Use when future values better represent the missing ones.

### BOCF (Baseline Observation Carried Forward)

Use initial baseline values for imputation, common in medical trials:

```
baseline = df['metric'][0]
df['metric'].fillna(baseline, inplace=True)
```

### WOCF (Worst Observation Carried Forward)

Assume worst-case scenario, useful in risk-sensitive domains:

```
worst = df['pain'].max()
df['pain'].fillna(worst, inplace=True)
```

---

## 🔁 Multiple Imputation

Iteratively guesses missing values, refining the imputation with each iteration.

### When to Use

- Best suited for **MAR** data
    
- Helpful for **categorical or numerical** variables
    
- Provides **more robust** and less biased estimates than single imputation
    

### How to Apply

Using `IterativeImputer` from `sklearn`:

```
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import pandas as pd
import numpy as np

# Example dataset with missing values
data = {
    'X': [5.4,13.8,14.7,17.6,np.nan,1.1,12.9,3.4,np.nan,10.2],
    'Y': [18,27.4,np.nan,18.3,49.6,48.9,np.nan,13.6,16.1,42.7],
    'Z': [7.6,4.6,4.2,np.nan,4.7,8.5,3.5,np.nan,1.8,4.7]
}
df = pd.DataFrame(data)

imp = IterativeImputer(max_iter=10, random_state=0)
df_complete = pd.DataFrame(imp.fit_transform(df), columns=df.columns)
```

---

## ⚠️ Disadvantages of Single Imputation

- **Bias risk**: Introduces assumptions about trends or values.
    
- **Over-smoothing**: Ignores natural variation.
    
- **Not ideal for MNAR**: Assumes missing data can be guessed from existing data.
    

---

## 📚 Best Practices

- Always **analyze patterns** of missingness.
    
- Use **domain knowledge** to categorize missingness.
    
- Prefer **multiple imputation** when appropriate.
    
- Document all assumptions and methods used.